language: python

dist: xenial

# Check against N & N-1 Spark versions
env:
  - SPARK_VERSION=2.4.1
  - SPARK_VERSION=2.4.3

python:
  - "3.6"
  - "3.7"

addons:
  apt:
    packages:
      - axel
  sonarcloud:
    organization: "astrolabsoftware"
    token:
      secure: "Q4ntGTDH7XnNe7t5r9tDrppY2lDnni5Mlpo0jKRDnyRuFcDvqTdcni1Vs3UghczI+qaoD/3zyfazHPV7OwIOGacgce8zsntt3g/8KeCoWvdKSVejhyHtjyA5NovwMr0otvvzyvPS4c+SuXbA9OJZNijARhOZ0aAYcNiuWsr9zOhCzTz0A5pt5T0rign1C2Fh5OZJQtfId6TZtvqQLeNei9z0v+d/SNOLLr59IT5IobXqnFmOPTnRkbGi+zzLm+eh+ifg/4pEtKeZfAe7Pbjd35R6vCFOhTs0z1IVsPPwVq0hXiwVDWB+mku0w+hiG0pNQzxkj1mcbF3i8szRlIih4kr1PuCJnsLe+lkMBSUUASt6kCBDPUDLylQDGWnEFA61BBQbeOY9WecfwFEphvJ9grzAyreUwxOQ1HD39i4I1UySyaBKtpiL02z7NzDWbzON41x99PG/V+U+uTpYq9qX3gIqwqE9vcZ0Nclr91ElKkMK+t1baO0Jibb8xGe0+/ilmrdKeOyJ2FUKE4b2MBnfOvIDMfH3TbmD1/bR87IZNfZoVjrv2krg0xVPszKthUPPHPOPLiow+GWRca40AVGaXw9hDG0nX7Yzm8WtwiYnCD/s3Oc+WXVhJY4bt4w6D+Z5ocYzOCZWXcWWmAUFUqwti0WPO9A6PKIE440pAcIms7Y="

before_install:
  - export FINK_HOME="/home/travis/build/${TRAVIS_REPO_SLUG}"
  # Install Java 8 for Xenial
  - source conf/java8_for_xenial.sh
  - export PATH=$HOME/.local/bin:${FINK_HOME}/bin:$PATH
  # Install HBase
  - source conf/install_hbase.sh

install:
  # Download spark
  - "[ -f spark ] || mkdir spark && cd spark && axel --quiet http://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop2.7.tgz && cd .."
  - tar -xf ./spark/spark-${SPARK_VERSION}-bin-hadoop2.7.tgz
  - export SPARK_HOME=`pwd`/spark-${SPARK_VERSION}-bin-hadoop2.7
  - echo "spark.yarn.jars=${SPARK_HOME}/jars/*.jar" > ${SPARK_HOME}/conf/spark-defaults.conf
  - export SPARKLIB=${SPARK_HOME}/python:${SPARK_HOME}/python/lib/py4j-0.10.7-src.zip
  - export PYTHONPATH="${SPARKLIB}:${FINK_HOME}:$PYTHONPATH"
  - export PATH="${SPARK_HOME}/bin:${SPARK_HOME}/sbin:${PATH}"

  # Download and Install Kafka
  - source conf/install_kafka.sh
  - export KAFKA_HOME=$( pwd )/kafka

  # Python deps
  - pip install --upgrade pip setuptools wheel
  - pip install -r requirements.txt

script:
  # For sonar to run correctly
  - git fetch --unshallow --quiet

  # Start Kafka Server and create a test topic
  - fink_kafka start
  - fink_kafka --create-topic fink_outstream

  # Execute tests and report coverage
  - fink start simulator -c ${FINK_HOME}/conf/fink.conf.travis
  - fink_test
  - bash <(curl -s https://codecov.io/bash)

  # Scan with sonarqube only if internal PR (i.e. no fork)
  # Note: this is unsatisfactory...
  - if [ $TRAVIS_PULL_REQUEST_SLUG == "astrolabsoftware/fink-broker" ]; then
    coverage xml -i;
    sonar-scanner;
    fi
