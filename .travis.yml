language: python

dist: xenial

# Check against N & N-1 Spark versions
env:
  - SPARK_VERSION=2.4.4
  - SPARK_VERSION=2.4.5

python:
  - "3.6"
  - "3.7"

addons:
  apt:
    packages:
      - axel
  sonarcloud:
    organization: "astrolabsoftware"
    token:
      secure: "Q4ntGTDH7XnNe7t5r9tDrppY2lDnni5Mlpo0jKRDnyRuFcDvqTdcni1Vs3UghczI+qaoD/3zyfazHPV7OwIOGacgce8zsntt3g/8KeCoWvdKSVejhyHtjyA5NovwMr0otvvzyvPS4c+SuXbA9OJZNijARhOZ0aAYcNiuWsr9zOhCzTz0A5pt5T0rign1C2Fh5OZJQtfId6TZtvqQLeNei9z0v+d/SNOLLr59IT5IobXqnFmOPTnRkbGi+zzLm+eh+ifg/4pEtKeZfAe7Pbjd35R6vCFOhTs0z1IVsPPwVq0hXiwVDWB+mku0w+hiG0pNQzxkj1mcbF3i8szRlIih4kr1PuCJnsLe+lkMBSUUASt6kCBDPUDLylQDGWnEFA61BBQbeOY9WecfwFEphvJ9grzAyreUwxOQ1HD39i4I1UySyaBKtpiL02z7NzDWbzON41x99PG/V+U+uTpYq9qX3gIqwqE9vcZ0Nclr91ElKkMK+t1baO0Jibb8xGe0+/ilmrdKeOyJ2FUKE4b2MBnfOvIDMfH3TbmD1/bR87IZNfZoVjrv2krg0xVPszKthUPPHPOPLiow+GWRca40AVGaXw9hDG0nX7Yzm8WtwiYnCD/s3Oc+WXVhJY4bt4w6D+Z5ocYzOCZWXcWWmAUFUqwti0WPO9A6PKIE440pAcIms7Y="

before_install:
  - export ORIGINAL_JAVA_HOME=$JAVA_HOME
  - export FINK_HOME="/home/travis/build/${TRAVIS_REPO_SLUG}"
  # Install Java 8 for Xenial
  - source conf/java8_for_xenial.sh
  - export PATH=$HOME/.local/bin:${FINK_HOME}/bin:$PATH
  # Install HBase
  - source conf/install_hbase.sh
  # Downlaod data
  - "cd datasim && source download_ztf_alert_data.sh && cd .."

install:
  # Download spark
  - "[ -f spark ] || mkdir spark && cd spark && axel --quiet http://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop2.7.tgz && cd .."
  - tar -xf ./spark/spark-${SPARK_VERSION}-bin-hadoop2.7.tgz
  - export SPARK_HOME=`pwd`/spark-${SPARK_VERSION}-bin-hadoop2.7
  - echo "spark.yarn.jars=${SPARK_HOME}/jars/*.jar" > ${SPARK_HOME}/conf/spark-defaults.conf
  - export SPARKLIB=${SPARK_HOME}/python:${SPARK_HOME}/python/lib/py4j-0.10.7-src.zip
  - export PYTHONPATH="${SPARKLIB}:${FINK_HOME}:$PYTHONPATH"
  - export PATH="${SPARK_HOME}/bin:${SPARK_HOME}/sbin:${PATH}"

  # Download and Install Kafka
  - source conf/install_kafka.sh
  - export KAFKA_HOME=$( pwd )/kafka

  # Python deps
  - pip install --upgrade pip setuptools wheel
  - source ./install_python_deps.sh 

  # Install fink-alert-simulator
  - git clone https://github.com/astrolabsoftware/fink-alert-simulator.git
  - export FINK_ALERT_SIMULATOR=${PWD}/fink-alert-simulator
  - export PYTHONPATH=$FINK_ALERT_SIMULATOR:$PYTHONPATH
  - export PATH=$FINK_ALERT_SIMULATOR/bin:$PATH

script:
  # For sonar to run correctly
  - git fetch --unshallow --quiet

  # Initialise paths for data and checkpoints
  - fink init -c ${FINK_HOME}/conf/fink.conf.travis

  # Start Kafka Server and create a test topic
  - fink_kafka start
  - fink_kafka --create-topic fink_outstream

  # Execute tests and report coverage
  - fink_simulator --docker -c ${FINK_HOME}/conf/fink_alert_simulator.conf
  - fink_test
  - bash <(curl -s https://codecov.io/bash)

  # Scan with sonarqube only if internal PR (i.e. no fork)
  # Note: this is unsatisfactory...
  - if [ $TRAVIS_REPO_SLUG == "astrolabsoftware/fink-broker" ]; then
    # restore java 11 before running sonar
    export JAVA_HOME=$ORIGINAL_JAVA_HOME
    echo $JAVA_HOME
    coverage xml -i;
    sonar-scanner;
    fi

notifications:
  slack:
    rooms:
      secure: "TH78sSD5a938BsRH1rI6nZtoC+5GV7GBTBS2sI4z1ySvdnJIcz26rfr3BtCpG8WYgrDgco3Dx3StIjRM0GlvuEVFDS2HbzfgjThmLdRU8u4fjaUqGHkJ4vZNXj3NUxK7lp0e2NGoNpp+meWJR7O1BYT4GmYoD03Gj4Bw4qTUDZlKLCM0rVsFcX7kSsoMhoYizGG89z7F8EsH95kFQTPp9ICYIX6qC+or45/9WphwuhKHa0Nt+8cRtJL8yttgEyhqLIORmZ91cqO2y6s4zgp0Ues+xAcQk3zBWhpnBmqqIMSTmd++mZLtRcXcAiqSAeG5B/cC05Trmzvtt9N/3ihdaXEFtnDTZ7aPj5ojSkr4Nu4VkFwMasQ8oj4vIwSnhtYN752B/ZnlkSEsSKB22kkq2ZzSC+3eatIU1qLDQrqJRVaMJ5Yihd1SRTFT7XORLFUbO8JpAOu/yjleRuklfvWH5kVBD3JiPVa4dq7N2f9YqWp9eXp1t9zH4YAGGbh86GaVILl7vOc1sTU4BPo5MjUA1QgqS9WUTUpaoG4nDx9Hwvs1PPfZTd/LU6VsC0gWAh/E2le7+QTlXt9r34u4a3oqMqxCk13ijuHJaBQv2wyac1VFrxXA06EAPE/zVAd9SUdgS3kMtUAtC0yCSCZePTpO0jag6hOJJhZMOPlmjzR4HW8="
