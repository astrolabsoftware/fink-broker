#!/bin/bash
# Copyright 2019-2025 AstroLab Software
# Author: Julien Peloton
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
## Script to launch the python test suite and measure the coverage.
## Must be launched as fink_test
set -e

SINFO="\xF0\x9F\x9B\x88"
SERROR="\xE2\x9D\x8C"
SSTOP="\xF0\x9F\x9B\x91"
SSTEP="\xF0\x9F\x96\xA7"
SDONE="\xE2\x9C\x85"

message_help="""
Run units and integration test suites of Fink\n\n
Usage:\n
    \tfink_test_rubin [--db-integration] [--unit-tests] [--stream-integration] [-h] [-c <conf>]\n\n

By default, nothing is run.\n
You can enable the database integration tests by specifying --db-integration.\n
You can enable the unit tests by specifying --unit-test.\n
You can enable the stream integration tests by specifying --stream-integration.\n
You can specify the configuration file followed by -c or else the default (${FINK_HOME}/conf/rubin/fink.conf.prod) will be used.\n
Use -h to display this help.
"""

# Grab the command line arguments
WITH_DB=false
WITH_UNITS=false
WITH_STREAM=false
while [ "$#" -gt 0 ]; do
  case "$1" in
    --stream-integration)
      WITH_STREAM=true
      shift 1
      ;;
    --db-integration)
      WITH_DB=true
      shift 1
      ;;
    --unit-tests)
      WITH_UNITS=true
      shift 1
      ;;
    -h)
      echo -e $message_help
      exit
      ;;
    -c)
      if [[ $2 == "" ]]; then
        echo "$1 requires an argument" >&2
        exit 1
      fi
      conf="$2"
      shift 2
      ;;
    -single_file)
      SINGLE_FILE=$2
      shift 2
      ;;
  esac
done

SURVEY=rubin

# Source configuration file for tests
if [[ -f $conf ]]; then
  echo "Reading custom test configuration file from " $conf
else
  conf=${FINK_HOME}/conf/${SURVEY}/fink.conf.dev
  echo "Reading the default test configuration file from " $conf
fi

source $conf

# Export variables for tester.py
export FINK_PACKAGES=$FINK_PACKAGES
export FINK_JARS=$FINK_JARS
export KAFKA_IPPORT_SIM=$KAFKA_IPPORT_SIM
export KAFKA_TOPIC="rubin_20250903_lsst_v10.0"

# TODO: change me
export NIGHT=20250412

# Add coverage_daemon to the pythonpath. See python/fink_broker/tester.py
export PYTHONPATH="${SPARK_HOME}/python/test_coverage:$PYTHONPATH"
export COVERAGE_PROCESS_START="${FINK_HOME}/.coveragerc"

# Create folder for log & artifacts
mkdir -p ${FINK_HOME}/broker_logs

if [ -n "${SINGLE_FILE}" ]; then
    echo "single file testing..."
    coverage run \
        --source=${FINK_HOME} \
        --rcfile ${FINK_HOME}/.coveragerc $SINGLE_FILE
    exit
fi

ensure_folder_has_been_created() {
  local path="$1"
  if [ ! -d ${path} ]; then
    echo -e "${SERROR} ${path} has not been created -- there might be a problem"
    exit 1
  else
    echo -e "${SDONE} ${path} created as expected"
  fi
}

check_table_output() {
  # make 101 as args
  local tablename="$1"
  shift
  local number="$@"
  output=`echo -e "count '$tablename'" | hbase shell -n`
  num_rows=$(echo "$output" | grep -o '[0-9]\+ row(s)' | grep -o '[0-9]\+')
  if [[ $num_rows != ${number} ]]; then
    echo -e "${SERROR} table ${tablename} has ${num_rows:-0} rows instead of ${number}"
  else
    echo -e "${SDONE} table ${tablename} is OK."
  fi
}

# Stream integration tests
if [[ "$WITH_STREAM" = true ]] ; then
  RESOURCES="-driver-memory 2g -executor-memory 2g -spark-cores-max 4 -spark-executor-cores 1"

  # Fire a stream
  fink_simulator -c ${FINK_HOME}/conf/${SURVEY}/fink_alert_simulator.conf

  # Connect the service to build the raw database from the stream
  ${FINK_HOME}/scheduler/rubin/launch_stream.sh -c ${conf} --poll_only -stop_at "`date -d '+30 seconds' +'%Y-%m-%d %H:%M'`" -night ${NIGHT}

  # Connect the service to build the science database from the raw one
  ${FINK_HOME}/scheduler/rubin/launch_stream.sh -c ${conf} --enrich_only -stop_at "`date -d '+5 minutes' +'%Y-%m-%d %H:%M'`" -night ${NIGHT}

  # Generate Kafka topics & HBase tables for classes
  ${FINK_HOME}/scheduler/rubin/launch_stream.sh -c ${conf} --distribute_only -stop_at "`date -d '+5 minutes' +'%Y-%m-%d %H:%M'`" -night ${NIGHT}

  # Ingest data to HBase
  ${FINK_HOME}/scheduler/rubin/launch_stream.sh -c ${conf} --ingest_only -stop_at "`date -d '+2 minutes' +'%Y-%m-%d %H:%M'`" -night ${NIGHT}

  echo "Sleeping 300 seconds to enable processing to produce output..."
  sleep 300
  ensure_folder_has_been_created "${ONLINE_DATA_PREFIX}/raw"
  ensure_folder_has_been_created "${ONLINE_DATA_PREFIX}/science"
  ensure_folder_has_been_created "${ONLINE_DATA_PREFIX}/science_checkpoint"
  ensure_folder_has_been_created "${ONLINE_DATA_PREFIX}/kafka_checkpoint"
  ensure_folder_has_been_created "${ONLINE_DATA_PREFIX}/hbase_checkpoint"

  # check we have all topics
  #echo "TOPICS FOUND:"
  #kafka-topics.sh --list --bootstrap-server localhost:9092

  # check HBase output
  check_table_output "rubin.diaObject" 94
  check_table_output "rubin.mpc_orbits" 8
  check_table_output "rubin.diaSource_static" 94
  check_table_output "rubin.diaSource_sso" 8
  check_table_output "rubin.cutouts" 101

  check_table_output "rubin.cataloged" 19
fi

# DB Integration tests
if [[ "$WITH_DB" = true ]] ; then
  # merge data
  fink_db -s ${SURVEY} -c $conf -night ${NIGHT} --merge
  ensure_folder_has_been_created $AGG_DATA_PREFIX/science

  # diaSource, diaObject only
  fink_db -s ${SURVEY} -c $conf -night ${NIGHT} --main_table

  # pixel128, cutouts, forcedSources only
  fink_db -s ${SURVEY} -c $conf -night ${NIGHT} --index_tables

  # check HBase output
  check_table_output "rubin.diaSource_static" 94
  check_table_output "rubin.diaSource_sso" 8
  check_table_output "rubin.diaObject" 94
  check_table_output "rubin.mpc_orbits" 8

  check_table_output "rubin.cutouts" 101
  check_table_output "rubin.pixel128" 94
  check_table_output "rubin.statistics" 2
  # check_table_output "rubin.forcedSources" 99
fi

if [[ "$WITH_UNITS" = true ]] ; then
  # Run the test suite on the modules assuming the integration
  # tests have been run (to build the databases)
  for i in ${FINK_HOME}/fink_broker/${SURVEY}/*.py
  do
    if [[ ${i##*/} = 'monitoring' ]] ; then
        echo "skip {i}"
    else
      coverage run \
          --source=${FINK_HOME} \
          --rcfile ${FINK_HOME}/.coveragerc $i
    fi
  done

  for i in ${FINK_HOME}/fink_broker/common/*.py
  do
    coverage run \
        --source=${FINK_HOME} \
        --rcfile ${FINK_HOME}/.coveragerc $i
  done
fi

# Combine individual reports in one
coverage combine

unset KAFKA_TOPIC
unset KAFKA_IPPORT_SIM
unset COVERAGE_PROCESS_START
unset FINK_PACKAGES
unset FINK_JARS

coverage report
coverage html
