#!/bin/bash
# Copyright 2019-2023 AstroLab Software
# Author: Julien Peloton
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
set -e

message_service="Available services are: checkstream, stream2raw, raw2science, distribution, science_archival, check_science_portal, stats"
message_conf="Typical configuration would be $PWD/conf/fink.conf"
message_help="""
Handle Kafka stream received by Apache Spark\n\n
Usage:\n
    \tto init paths     : fink init [-h] [-c <conf>]\n
    \tto start a service: fink start <service> [-h] [-c <conf>] [--simulator]\n
    \tto stop a service : fink stop  <service> [-h] [-c <conf>]\n
    \tto show services  : fink show [-h] \n\n

To get this help: \n
\tfink \n\n

To get help for a service: \n
\tfink start <service> -h\n\n

$message_service\n
$message_conf\n\n

To see the running processes: \n
\tfink show \n\n

To initialise data folders and paths: \n
\tfink init \n\n

To stop a service or all running processes: \n
\tfink stop <service or all> \n
"""
# Show help if no arguments is given
if [[ $1 == "" ]]; then
  echo -e $message_help
  exit 1
fi

# Grab the command line arguments
while [ "$#" -gt 0 ]; do
  case "$1" in
    "start"|"stop")
        MODE="$1"
        if [[ $2 == "" || $2 == "-c" ]]; then
          echo "$1 requires an argument. ${message_service}" >&2
          exit 1
        fi
        service="$2"
        shift 2
        ;;
    "show")
        nservice=$(ps ax | grep -i 'fink start' | grep -v grep | wc -l)
        echo -e "$nservice Fink service(s) running: "
        ps aux | head -1
        ps aux | grep -i 'fink start' | grep -v grep
        echo "Use <fink stop service_name> to stop a service."
        exit
        ;;
    "init")
        INITPATH=true
        shift 1
        ;;
    -h)
        HELP_ON_SERVICE="-h"
        shift 1
        ;;
    -c)
        if [[ $2 == "" || $2 == "-s" ]]; then
          echo "$1 requires an argument. ${message_conf}" >&2
          exit 1
        fi
        conf="$2"
        shift 2
        ;;
    --conf=*)
        conf="${1#*=}"
        shift 1
        ;;
    --conf)
        echo "$1 requires an argument" >&2
        exit 1
        ;;
    --simulator)
        SIM_ONLY=true
        shift 1
        ;;
    --version)
        python3 -c "import fink_broker; print(fink_broker.__version__)"
        exit
        ;;
    --night)
      NIGHT="$2"
      shift 2
      ;;
    --topic)
      KAFKA_TOPIC="$2"
      shift 2
      ;;
    --index_table)
      INDEXTABLE="$2"
      shift 2
      ;;
    --tns_folder)
      TNS_FOLDER="$2"
      shift 2
      ;;
    --tns_sandbox)
      TNS_SANDBOX=true
      shift 1
      ;;
    --elasticc)
      ELASTICC=true
      shift 1
      ;;
    --exit_after)
        EXIT_AFTER="-exit_after $2"
        shift 2
        ;;
    -*)
        echo "unknown option: $1" >&2
        exit 1
        ;;
    *)
        echo "unknown argument: $1" >&2
        exit 1
        ;;
  esac
done

# Check if the conf file exists
if [[ -f $conf ]]; then
  echo "Reading custom Fink configuration file from " $conf
  source $conf
else
  echo "Reading default Fink conf from " ${FINK_HOME}/conf/fink.conf
  source ${FINK_HOME}/conf/fink.conf
fi

# Initialise paths to store data and checkpoints
if [[ "$INITPATH" = true ]] ; then
  if [[ "$FS_KIND" = "local" ]] ; then
    finkdir='mkdir'
  elif [[ "$FS_KIND" = "hdfs" ]] ; then
    finkdir='hdfs dfs -mkdir'
  else
    echo "$FS_KIND file system not understood! Currently available: FS_KIND={local, hdfs}"
    exit 1
  fi
  $finkdir $ONLINE_DATA_PREFIX
  $finkdir $ONLINE_DATA_PREFIX/raw
  $finkdir $ONLINE_DATA_PREFIX/science
  $finkdir $ONLINE_DATA_PREFIX/raw_checkpoint
  $finkdir $ONLINE_DATA_PREFIX/science_checkpoint
  $finkdir $ONLINE_DATA_PREFIX/kafka_checkpoint
  echo "Data will be stored under $ONLINE_DATA_PREFIX"
  exit
fi

if [[ "$SIM_ONLY" = true ]] ; then
  KAFKA_IPPORT=$KAFKA_IPPORT_SIM
fi

# Stop services
if [[ $MODE == "stop" ]]; then
  if [[ $service == "all" ]]; then
    SIGNAL=${SIGNAL:-TERM}

    # Kill instance of HBase connector running if needed
    PIDS=$(ps ax | grep -i 'fink' | grep "hbase" | awk '{print $1}')

    if [ -z "$PIDS" ]; then
      echo "No HBase connection to stop"
    else
      echo "Stopping HBase connection..."
      kill -s $SIGNAL $PIDS
    fi

    # Kill other services
    PIDS=$(ps ax | grep -i 'fink' | awk '{print $1}')
    if [ -z "$PIDS" ]; then
      echo "No fink service(s) to stop"
    else
      echo "Stopping all services..."
      kill -s $SIGNAL $PIDS
    fi
  else
    SIGNAL=${SIGNAL:-TERM}

    # Kill instance of HBase connector running if needed
    if [[ $service == "raw2science" ]]; then
      PIDS=$(ps ax | grep -i 'fink' | grep "hbase" | awk '{print $1}')

      if [ -z "$PIDS" ]; then
        echo "No fink $service service to stop"
      else
        echo "Stopping $service (HBase connection)..."
        kill -s $SIGNAL $PIDS
      fi
    fi

    PIDS=$(ps ax | grep -i 'fink' | grep $service | awk '{print $1}')

    if [ -z "$PIDS" ]; then
      echo "No fink $service service to stop"
    else
      echo "Stopping $service..."
      kill -s $SIGNAL $PIDS
    fi
  fi
  exit 1
fi

# Grab Fink and Python version numbers
FINK_VERSION=`fink --version`
PYTHON_VERSION=`python -c "import platform; print(platform.python_version()[:3])"`

# Package fink_broker & userfilters modules
PYTHON_EXTRA_FILE=""
if [[ $DEPLOY_FINK_PYTHON == "true" ]]; then
  cd $FINK_HOME
  python3 setup.py bdist_egg
  PYTHON_EXTRA_FILE="--py-files ${FINK_HOME}/dist/fink_broker-${FINK_VERSION}-py${PYTHON_VERSION}.egg"
  echo "Distributing ${PYTHON_EXTRA_FILE}"
  cd -
fi

if [[ $service == "checkstream" ]]; then
  # Monitor the stream of alerts
  spark-submit --master ${SPARK_MASTER} \
    --packages ${FINK_PACKAGES} ${PYTHON_EXTRA_FILE} \
    ${SECURED_KAFKA_CONFIG} ${EXTRA_SPARK_CONFIG} \
    ${FINK_HOME}/bin/checkstream.py ${HELP_ON_SERVICE} -servers ${KAFKA_IPPORT} \
    -topic ${KAFKA_TOPIC} -startingoffsets_stream ${KAFKA_STARTING_OFFSET} \
    -log_level ${LOG_LEVEL} ${EXIT_AFTER}
elif [[ $service == "stream2raw" ]]; then

  # Store the stream of alerts
  spark-submit --master ${SPARK_MASTER} \
    --packages ${FINK_PACKAGES} ${PYTHON_EXTRA_FILE} \
    ${SECURED_KAFKA_CONFIG} ${EXTRA_SPARK_CONFIG} \
    ${FINK_HOME}/bin/stream2raw.py ${HELP_ON_SERVICE} -producer ${PRODUCER} -servers ${KAFKA_IPPORT} -topic ${KAFKA_TOPIC} \
    -schema ${FINK_ALERT_SCHEMA} -startingoffsets_stream ${KAFKA_STARTING_OFFSET} \
    -online_data_prefix ${ONLINE_DATA_PREFIX} \
    -tinterval ${FINK_TRIGGER_UPDATE} -log_level ${LOG_LEVEL} ${EXIT_AFTER}
elif [[ $service == "raw2science" ]]; then

  # Store the stream of alerts
  spark-submit --master ${SPARK_MASTER} \
    --packages ${FINK_PACKAGES} \
    --jars ${FINK_JARS} \
    ${PYTHON_EXTRA_FILE} \
    ${SECURED_KAFKA_CONFIG} ${EXTRA_SPARK_CONFIG} \
    ${FINK_HOME}/bin/raw2science.py ${HELP_ON_SERVICE} \
    -producer ${PRODUCER} \
    -online_data_prefix ${ONLINE_DATA_PREFIX} \
    -tinterval ${FINK_TRIGGER_UPDATE} \
    -night ${NIGHT} \
    -log_level ${LOG_LEVEL} ${EXIT_AFTER}
elif [[ $service == "distribution" ]]; then
  if [[ $ELASTICC == true ]]; then
    SCRIPT=${FINK_HOME}/bin/distribute_elasticc.py
  else
    SCRIPT=${FINK_HOME}/bin/distribute.py
  fi
  # Read configuration for redistribution
  source ${FINK_HOME}/conf/fink.conf.distribution
  # Start the Spark Producer
  spark-submit --master ${SPARK_MASTER} \
  --packages ${FINK_PACKAGES} \
  --jars ${FINK_JARS} ${PYTHON_EXTRA_FILE} ${EXTRA_SPARK_CONFIG} \
  --files ${FINK_HOME}/conf/fink_kafka_producer_jaas.conf \
  --driver-java-options "-Djava.security.auth.login.config=${FINK_PRODUCER_JAAS}" \
  --conf "spark.driver.extraJavaOptions=-Djava.security.auth.login.config=${FINK_PRODUCER_JAAS}" \
  --conf "spark.executor.extraJavaOptions=-Djava.security.auth.login.config=${FINK_PRODUCER_JAAS}" \
  $SCRIPT ${HELP_ON_SERVICE} \
  -producer ${PRODUCER} \
  -online_data_prefix ${ONLINE_DATA_PREFIX} \
  -distribution_servers ${DISTRIBUTION_SERVERS} \
  -distribution_schema ${DISTRIBUTION_SCHEMA} \
  -substream_prefix ${SUBSTREAM_PREFIX} \
  -tinterval ${FINK_TRIGGER_UPDATE} \
  -night ${NIGHT} \
  -log_level ${LOG_LEVEL} ${EXIT_AFTER}
elif [[ $service == "merge" ]]; then
  spark-submit --master ${SPARK_MASTER} \
  --packages ${FINK_PACKAGES} \
  --jars ${FINK_JARS} ${PYTHON_EXTRA_FILE} ${EXTRA_SPARK_CONFIG} \
  ${FINK_HOME}/bin/merge_ztf_night.py ${HELP_ON_SERVICE} \
  -online_data_prefix ${ONLINE_DATA_PREFIX} \
  -agg_data_prefix ${AGG_DATA_PREFIX} \
  -night ${NIGHT} \
  -log_level ${LOG_LEVEL} ${EXIT_AFTER}
elif [[ $service == "stats" ]]; then
  spark-submit --master ${SPARK_MASTER} \
  --packages ${FINK_PACKAGES} \
  --jars ${FINK_JARS} ${PYTHON_EXTRA_FILE} ${EXTRA_SPARK_CONFIG} \
  ${FINK_HOME}/bin/daily_stats.py ${HELP_ON_SERVICE} \
  -agg_data_prefix ${AGG_DATA_PREFIX} \
  -night ${NIGHT} \
  -science_db_catalogs ${SCIENCE_DB_CATALOGS} \
  -log_level ${LOG_LEVEL} ${EXIT_AFTER}
elif [[ $service == "reprocess_night" ]]; then
  spark-submit --master ${SPARK_MASTER} \
  --packages ${FINK_PACKAGES} \
  --jars ${FINK_JARS} ${PYTHON_EXTRA_FILE} ${EXTRA_SPARK_CONFIG} \
  ${FINK_HOME}/bin/raw2science_batch.py ${HELP_ON_SERVICE} \
  -agg_data_prefix ${AGG_DATA_PREFIX} \
  -night ${NIGHT} \
  -log_level ${LOG_LEVEL}
elif [[ $service == "science_archival" ]]; then
  spark-submit --master ${SPARK_MASTER} \
  --packages ${FINK_PACKAGES} \
  --jars ${FINK_JARS} ${PYTHON_EXTRA_FILE} ${EXTRA_SPARK_CONFIG} \
  ${FINK_HOME}/bin/science_archival.py ${HELP_ON_SERVICE} \
  -agg_data_prefix ${AGG_DATA_PREFIX} \
  -science_db_name ${SCIENCE_DB_NAME} \
  -science_db_catalogs ${SCIENCE_DB_CATALOGS} \
  -night ${NIGHT} \
  -log_level ${LOG_LEVEL} ${EXIT_AFTER}
elif [[ $service == "index_archival" ]]; then
  # Set dummy location if TNS_FOLDER was not provided
  # This is only needed by the tns_jd table
  if [[ $TNS_FOLDER == "" ]]; then
    TNS_FOLDER="/tmp"
  fi
  spark-submit --master ${SPARK_MASTER} \
  --packages ${FINK_PACKAGES} \
  --jars ${FINK_JARS} ${PYTHON_EXTRA_FILE} ${EXTRA_SPARK_CONFIG} \
  ${FINK_HOME}/bin/index_archival.py ${HELP_ON_SERVICE} \
  -agg_data_prefix ${AGG_DATA_PREFIX} \
  -science_db_name ${SCIENCE_DB_NAME} \
  -science_db_catalogs ${SCIENCE_DB_CATALOGS} \
  -night ${NIGHT} \
  -index_table ${INDEXTABLE} \
  -tns_folder ${TNS_FOLDER} \
  -log_level ${LOG_LEVEL} ${EXIT_AFTER}
elif [[ $service == "index_sso_cand_archival" ]]; then
  spark-submit --master ${SPARK_MASTER} \
  --packages ${FINK_PACKAGES} \
  --jars ${FINK_JARS} ${PYTHON_EXTRA_FILE} ${EXTRA_SPARK_CONFIG} \
  ${FINK_HOME}/bin/index_sso_cand_archival.py ${HELP_ON_SERVICE} \
  -science_db_name ${SCIENCE_DB_NAME} \
  -science_db_catalogs ${SCIENCE_DB_CATALOGS} \
  -night ${NIGHT} \
  -fink_fat_output ${FINK_FAT_OUTPUT} \
  -log_level ${LOG_LEVEL} ${EXIT_AFTER}
elif [[ $service == "object_archival" ]]; then
  spark-submit --master ${SPARK_MASTER} \
  --packages ${FINK_PACKAGES} \
  --jars ${FINK_JARS} ${PYTHON_EXTRA_FILE} ${EXTRA_SPARK_CONFIG} \
  ${FINK_HOME}/bin/object_archival.py ${HELP_ON_SERVICE} \
  -science_db_name ${SCIENCE_DB_NAME} \
  -science_db_catalogs ${SCIENCE_DB_CATALOGS} \
  -night ${NIGHT} \
  -log_level ${LOG_LEVEL} ${EXIT_AFTER}
elif [[ $service == "push_to_tns" ]]; then
  # Check if we just need to test
  if [[ $TNS_SANDBOX = true ]] ; then
    TNS_SANDBOX="--tns_sandbox"
  else
    TNS_SANDBOX=""
  fi

  spark-submit --master ${SPARK_MASTER} \
  --packages ${FINK_PACKAGES} \
  --jars ${FINK_JARS} ${PYTHON_EXTRA_FILE} ${EXTRA_SPARK_CONFIG} \
  ${FINK_HOME}/bin/push_to_tns.py ${HELP_ON_SERVICE} \
  -agg_data_prefix ${AGG_DATA_PREFIX} \
  -night ${NIGHT} \
  -tns_folder ${TNS_FOLDER} ${TNS_SANDBOX} \
  -log_level ${LOG_LEVEL} ${EXIT_AFTER}
elif [[ $service == "anomaly_archival" ]]; then
  spark-submit --master ${SPARK_MASTER} \
  --packages ${FINK_PACKAGES} \
  --jars ${FINK_JARS} ${PYTHON_EXTRA_FILE} ${EXTRA_SPARK_CONFIG} \
  ${FINK_HOME}/bin/anomaly_archival.py ${HELP_ON_SERVICE} \
  -agg_data_prefix ${AGG_DATA_PREFIX} \
  -night ${NIGHT} \
  -log_level ${LOG_LEVEL} ${EXIT_AFTER}
elif [[ $service == "check_science_portal" ]]; then
  spark-submit --master ${SPARK_MASTER} \
  --packages ${FINK_PACKAGES} \
  --jars ${FINK_JARS} ${PYTHON_EXTRA_FILE} ${EXTRA_SPARK_CONFIG} \
  ${FINK_HOME}/bin/access_science_db.py ${HELP_ON_SERVICE} \
  -science_db_name ${SCIENCE_DB_NAME} \
  -science_db_catalogs ${SCIENCE_DB_CATALOGS} \
  -log_level ${LOG_LEVEL} ${EXIT_AFTER}
elif [[ $service == "distribution_test" ]]; then
  # Read configuration for redistribution
  source ${FINK_HOME}/conf/fink.conf.distribution
  # Start a spark consumer to test the distribution
  spark-submit --master ${SPARK_MASTER} \
  --packages ${FINK_PACKAGES} \
  --jars ${FINK_JARS} ${PYTHON_EXTRA_FILE} \
  --files ${FINK_HOME}/conf/fink_kafka_consumer_jaas.conf \
  --driver-java-options "-Djava.security.auth.login.config=${FINK_TEST_CONSUMER_JAAS}" \
  --conf "spark.driver.extraJavaOptions=-Djava.security.auth.login.config=${FINK_TEST_CONSUMER_JAAS}" \
  --conf "spark.executor.extraJavaOptions=-Djava.security.auth.login.config=${FINK_TEST_CONSUMER_JAAS}" \
  ${FINK_HOME}/bin/distribution_test.py ${HELP_ON_SERVICE} ${EXIT_AFTER} \
  -distribution_servers ${DISTRIBUTION_SERVERS} \
  -distribution_topic ${DISTRIBUTION_TOPIC} \
  -distribution_schema ${DISTRIBUTION_SCHEMA} -log_level ${LOG_LEVEL}
elif [[ $service == "save_schema" ]]; then
  spark-submit --master ${SPARK_MASTER} \
  --packages ${FINK_PACKAGES} \
  --jars ${FINK_JARS} ${PYTHON_EXTRA_FILE} ${EXTRA_SPARK_CONFIG} \
  ${FINK_HOME}/bin/save_distribution_schema.py ${HELP_ON_SERVICE} ${EXIT_AFTER} \
  -agg_data_prefix ${AGG_DATA_PREFIX} \
  -night ${NIGHT} -log_level ${LOG_LEVEL}
else
  # In case you give an unknown service
  echo "unknown service: $service" >&2
  echo $message_service
  exit 1
fi
