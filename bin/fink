#!/bin/bash
# Copyright 2019-2025 AstroLab Software
# Author: Julien Peloton
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
set -e

message_service="Available services are: poll, enrich, distribute, merge, archive"
message_conf="Typical configuration would be $PWD/conf/fink.conf"
message_help="""
Handle Kafka stream received by Apache Spark\n\n
Usage:\n
    \tto start a service: fink start <service> [-h] [-s <survey>] [-c <conf>] [--simulator]\n
    \tto stop a service : fink stop  <service> [-h] [-c <conf>]\n
    \tto show services  : fink show [-h] \n\n

To get this help: \n
\tfink \n\n

To get help for a service: \n
\tfink start <service> -h\n\n

$message_service\n
$message_conf\n\n

To see the running processes: \n
\tfink show \n\n

To stop a service or all running processes: \n
\tfink stop <service or all> \n
"""
# Show help if no arguments is given
if [[ $1 == "" ]]; then
  echo -e $message_help
  exit 1
fi

# Grab the command line arguments
while [ "$#" -gt 0 ]; do
  case "$1" in
    "start"|"stop")
        MODE="$1"
        if [[ $2 == "" || $2 == "-c" ]]; then
          echo "$1 requires an argument. ${message_service}" >&2
          exit 1
        fi
        service="$2"
        shift 2
        ;;
    "show")
        nservice=$(ps ax | grep -i 'fink start' | grep -v grep | wc -l)
        echo -e "$nservice Fink service(s) running: "
        ps aux | head -1
        ps aux | grep -i 'fink start' | grep -v grep
        ps aux | grep -i 'gcn_stream start' | grep -v grep
        echo "Use <fink stop service_name> to stop a service."
        exit
        ;;
    -h)
        HELP_ON_SERVICE="-h"
        shift 1
        ;;
    -s)
	SURVEY=$2
	shift 2
	;;
    -c)
        if [[ $2 == "" || $2 == "-s" ]]; then
          echo "$1 requires an argument. ${message_conf}" >&2
          exit 1
        fi
        conf="$2"
        shift 2
        ;;
    --conf=*)
        conf="${1#*=}"
        shift 1
        ;;
    --conf)
        echo "$1 requires an argument" >&2
        exit 1
        ;;
    -conf_distribution)
        conf_distribution="$2"
        shift 2
        ;;
    --simulator)
        SIM_ONLY=true
        shift 1
        ;;
    --version)
        python3 -c "import fink_broker; print(fink_broker.__version__)"
        exit
        ;;
    --night)
      NIGHT="$2"
      shift 2
      ;;
    --topic)
      KAFKA_TOPIC="$2"
      shift 2
      ;;
    --index_table)
      INDEXTABLE="$2"
      shift 2
      ;;
    --tns_folder)
      TNS_FOLDER="$2"
      shift 2
      ;;
    --tns_raw_output)
      TNS_RAW_OUTPUT="$2"
      shift 2
      ;;
    --tns_sandbox)
      TNS_SANDBOX=true
      shift 1
      ;;
    --elasticc)
      ELASTICC=true
      shift 1
      ;;
    --noscience)
      NOSCIENCE=true
      shift 1
      ;;
    --exit_after)
        EXIT_AFTER="-exit_after $2"
        shift 2
        ;;
    -*)
        echo "unknown option: $1" >&2
        exit 1
        ;;
    *)
        echo "unknown argument: $1" >&2
        exit 1
        ;;
  esac
done

# Check if the conf file exists
if [[ -f $conf ]]; then
  echo "Reading custom Fink configuration file from " $conf
  source $conf
else
  echo "Reading default Fink conf from " ${FINK_HOME}/conf/fink.conf
  source ${FINK_HOME}/conf/fink.conf
fi

if [[ "$SIM_ONLY" = true ]] ; then
  KAFKA_IPPORT=$KAFKA_IPPORT_SIM
fi

# Stop a service
if [[ $MODE == "stop" ]]; then
  if [[ $service == "all" ]]; then
    pkill -f stream2raw
    pkill -f raw2science
    pkill -f distribute
  else
    pkill -f $service
  fi
  exit 1
fi

# Grab Fink and Python version numbers
FINK_VERSION=`fink --version`
PYTHON_VERSION=`python -c "import platform; print(platform.python_version()[:3])"`

# Package fink_broker & userfilters modules
PYTHON_EXTRA_FILE=""
if [[ $DEPLOY_FINK_PYTHON == "true" ]]; then
  cd $FINK_HOME
  python3 setup.py bdist_egg
  PYTHON_EXTRA_FILE="--py-files ${FINK_HOME}/dist/fink_broker-${FINK_VERSION}-py${PYTHON_VERSION}.egg"
  echo "Distributing ${PYTHON_EXTRA_FILE}"
  cd -
fi

if [[ $service == "stream2raw" ]]; then

  # Store the stream of alerts
  spark-submit --master ${SPARK_MASTER} \
    --packages ${FINK_PACKAGES} ${PYTHON_EXTRA_FILE} \
    ${SECURED_KAFKA_CONFIG} ${EXTRA_SPARK_CONFIG} \
    ${FINK_HOME}/bin/$SURVEY/stream2raw.py ${HELP_ON_SERVICE} \
    -producer ${PRODUCER} \
    -servers ${KAFKA_IPPORT} \
    -topic ${KAFKA_TOPIC} \
    -night ${NIGHT} \
    -schema ${FINK_ALERT_SCHEMA} \
    -startingoffsets_stream ${KAFKA_STARTING_OFFSET} \
    -max_offsets_per_trigger ${MAX_OFFSETS_PER_TRIGGER} \
    -online_data_prefix ${ONLINE_DATA_PREFIX} \
    -tinterval ${FINK_TRIGGER_UPDATE} \
    -log_level ${LOG_LEVEL} ${EXIT_AFTER}
elif [[ $service == "raw2science" ]]; then
  if [[ $NOSCIENCE == true ]]; then
    NOSCIENCE="--noscience"
  else
    NOSCIENCE=""
  fi

  if [[ $TNS_RAW_OUTPUT ]]; then
    TNS_OPTION="-tns_raw_output ${TNS_RAW_OUTPUT}"
  else
    TNS_OPTION=""
  fi

  # Store the stream of alerts
  spark-submit --master ${SPARK_MASTER} \
    --packages ${FINK_PACKAGES} \
    --jars ${FINK_JARS} \
    ${PYTHON_EXTRA_FILE} \
    ${SECURED_KAFKA_CONFIG} ${EXTRA_SPARK_CONFIG} \
    ${FINK_HOME}/bin/$SURVEY/raw2science.py ${HELP_ON_SERVICE} \
    -producer ${PRODUCER} \
    -online_data_prefix ${ONLINE_DATA_PREFIX} \
    -tinterval ${FINK_TRIGGER_UPDATE} \
    -night ${NIGHT} \
    -mmconfigpath ${FINK_MM_CONFIG} \
    -log_level ${LOG_LEVEL} ${TNS_OPTION} ${NOSCIENCE} ${EXIT_AFTER}
elif [[ $service == "distribution" ]]; then

  if [[ $NOSCIENCE == true ]]; then
    NOSCIENCE="--noscience"
  else
    NOSCIENCE=""
  fi

  # Check if the conf file exists
  if [[ -f $conf_distribution ]]; then
    echo "Reading custom Fink distribution configuration file from " $conf_distribution
    source $conf_distribution
  else
    echo "Reading default Fink distribution conf from " ${FINK_HOME}/conf/fink.conf.distribution
    source ${FINK_HOME}/conf/fink.conf.distribution
  fi
  # Start the Spark Producer
  spark-submit --master ${SPARK_MASTER} \
  --packages ${FINK_PACKAGES} \
  --jars ${FINK_JARS} ${PYTHON_EXTRA_FILE} ${EXTRA_SPARK_CONFIG} \
  --files ${FINK_HOME}/conf/fink_kafka_producer_jaas.conf \
  --driver-java-options "-Djava.security.auth.login.config=${FINK_PRODUCER_JAAS}" \
  --conf "spark.driver.extraJavaOptions=-Djava.security.auth.login.config=${FINK_PRODUCER_JAAS}" \
  --conf "spark.executor.extraJavaOptions=-Djava.security.auth.login.config=${FINK_PRODUCER_JAAS}" \
  --conf spark.executorEnv.KNWEBHOOK=${KNWEBHOOK}\
  --conf spark.executorEnv.KNWEBHOOK_FINK=${KNWEBHOOK_FINK}\
  --conf spark.executorEnv.KNWEBHOOK_AMA_CL=${KNWEBHOOK_AMA_CL}\
  --conf spark.executorEnv.KNWEBHOOK_AMA_GALAXIES=${KNWEBHOOK_AMA_GALAXIES}\
  --conf spark.executorEnv.KNWEBHOOK_DWF=${KNWEBHOOK_DWF}\
  --conf spark.executorEnv.FINK_TG_TOKEN=${FINK_TG_TOKEN} \
  ${FINK_HOME}/bin/$SURVEY/distribute.py ${HELP_ON_SERVICE} \
  -producer ${PRODUCER} \
  -online_data_prefix ${ONLINE_DATA_PREFIX} \
  -distribution_servers ${DISTRIBUTION_SERVERS} \
  -distribution_schema ${DISTRIBUTION_SCHEMA} \
  -kafka_sasl_username ${KAFKA_SASL_USERNAME} \
  -kafka_sasl_password ${KAFKA_SASL_PASSWORD} \
  -kafka_buffer_memory ${KAFKA_BUFFER_MEMORY} \
  -kafka_delivery_timeout_ms ${KAFKA_DELIVERY_TIMEOUT_MS} \
  -substream_prefix ${SUBSTREAM_PREFIX} \
  -tinterval ${FINK_TRIGGER_UPDATE} \
  -mmconfigpath ${FINK_MM_CONFIG} \
  -night ${NIGHT} \
  -log_level ${LOG_LEVEL} ${NOSCIENCE} ${EXIT_AFTER}
elif [[ $service == "merge" ]]; then
  spark-submit --master ${SPARK_MASTER} \
  --packages ${FINK_PACKAGES} \
  --jars ${FINK_JARS} ${PYTHON_EXTRA_FILE} ${EXTRA_SPARK_CONFIG} \
  ${FINK_HOME}/bin/$SURVEY/merge_ztf_night.py ${HELP_ON_SERVICE} \
  -online_data_prefix ${ONLINE_DATA_PREFIX} \
  -agg_data_prefix ${AGG_DATA_PREFIX} \
  -night ${NIGHT} \
  -log_level ${LOG_LEVEL} ${EXIT_AFTER}
elif [[ $service == "stats" ]]; then
  spark-submit --master ${SPARK_MASTER} \
  --packages ${FINK_PACKAGES} \
  --jars ${FINK_JARS} ${PYTHON_EXTRA_FILE} ${EXTRA_SPARK_CONFIG} \
  ${FINK_HOME}/bin/$SURVEY/daily_stats.py ${HELP_ON_SERVICE} \
  -agg_data_prefix ${AGG_DATA_PREFIX} \
  -night ${NIGHT} \
  -science_db_catalogs ${SCIENCE_DB_CATALOGS} \
  -log_level ${LOG_LEVEL} ${EXIT_AFTER}
elif [[ $service == "science_archival" ]]; then
  spark-submit --master ${SPARK_MASTER} \
  --packages ${FINK_PACKAGES} \
  --jars ${FINK_JARS} ${PYTHON_EXTRA_FILE} ${EXTRA_SPARK_CONFIG} \
  ${FINK_HOME}/bin/$SURVEY/science_archival.py ${HELP_ON_SERVICE} \
  -agg_data_prefix ${AGG_DATA_PREFIX} \
  -science_db_name ${SCIENCE_DB_NAME} \
  -science_db_catalogs ${SCIENCE_DB_CATALOGS} \
  -night ${NIGHT} \
  -log_level ${LOG_LEVEL} ${EXIT_AFTER}
elif [[ $service == "images_archival" ]]; then
  spark-submit --master ${SPARK_MASTER} \
  --packages ${FINK_PACKAGES} \
  --jars ${FINK_JARS} ${PYTHON_EXTRA_FILE} ${EXTRA_SPARK_CONFIG} \
  ${FINK_HOME}/bin/$SURVEY/images_archival.py ${HELP_ON_SERVICE} \
  -agg_data_prefix ${AGG_DATA_PREFIX} \
  -science_db_name ${SCIENCE_DB_NAME} \
  -science_db_catalogs ${SCIENCE_DB_CATALOGS} \
  -night ${NIGHT} \
  -log_level ${LOG_LEVEL} ${EXIT_AFTER}
elif [[ $service == "index_archival" ]]; then
  # Set dummy location if TNS_FOLDER was not provided
  # This is only needed by the tns_jd table
  if [[ $TNS_FOLDER == "" ]]; then
    TNS_FOLDER="/tmp"
  fi
  spark-submit --master ${SPARK_MASTER} \
  --packages ${FINK_PACKAGES} \
  --jars ${FINK_JARS} ${PYTHON_EXTRA_FILE} ${EXTRA_SPARK_CONFIG} \
  ${FINK_HOME}/bin/$SURVEY/index_archival.py ${HELP_ON_SERVICE} \
  -agg_data_prefix ${AGG_DATA_PREFIX} \
  -science_db_name ${SCIENCE_DB_NAME} \
  -science_db_catalogs ${SCIENCE_DB_CATALOGS} \
  -night ${NIGHT} \
  -index_table ${INDEXTABLE} \
  -tns_folder ${TNS_FOLDER} \
  -log_level ${LOG_LEVEL} ${EXIT_AFTER}
elif [[ $service == "index_sso_cand_archival" ]]; then
  spark-submit --master ${SPARK_MASTER} \
  --packages ${FINK_PACKAGES} \
  --jars ${FINK_JARS} ${PYTHON_EXTRA_FILE} ${EXTRA_SPARK_CONFIG} \
  ${FINK_HOME}/bin/$SURVEY/index_sso_cand_archival.py ${HELP_ON_SERVICE} \
  -science_db_name ${SCIENCE_DB_NAME} \
  -science_db_catalogs ${SCIENCE_DB_CATALOGS} \
  -night ${NIGHT} \
  -fink_fat_output ${FINK_FAT_OUTPUT} \
  -log_level ${LOG_LEVEL} ${EXIT_AFTER}
elif [[ $service == "push_to_tns" ]]; then
  # Check if we just need to test
  if [[ $TNS_SANDBOX = true ]] ; then
    TNS_SANDBOX="--tns_sandbox"
  else
    TNS_SANDBOX=""
  fi

  spark-submit --master ${SPARK_MASTER} \
  --packages ${FINK_PACKAGES} \
  --jars ${FINK_JARS} ${PYTHON_EXTRA_FILE} ${EXTRA_SPARK_CONFIG} \
  ${FINK_HOME}/bin/$SURVEY/push_to_tns.py ${HELP_ON_SERVICE} \
  -agg_data_prefix ${AGG_DATA_PREFIX} \
  -night ${NIGHT} \
  -tns_folder ${TNS_FOLDER} ${TNS_SANDBOX} \
  -log_level ${LOG_LEVEL} ${EXIT_AFTER}
elif [[ $service == "anomaly_archival" ]]; then
  spark-submit --master ${SPARK_MASTER} \
  --packages ${FINK_PACKAGES} \
  --jars ${FINK_JARS} ${PYTHON_EXTRA_FILE} ${EXTRA_SPARK_CONFIG} \
  ${FINK_HOME}/bin/$SURVEY/anomaly_archival.py ${HELP_ON_SERVICE} \
  -agg_data_prefix ${AGG_DATA_PREFIX} \
  -science_db_name ${SCIENCE_DB_NAME} \
  -science_db_catalogs ${SCIENCE_DB_CATALOGS} \
  -night ${NIGHT} \
  -log_level ${LOG_LEVEL} ${EXIT_AFTER}
elif [[ $service == "al_loop" ]]; then
  spark-submit --master ${SPARK_MASTER} \
  --packages ${FINK_PACKAGES} \
  --jars ${FINK_JARS} ${PYTHON_EXTRA_FILE} ${EXTRA_SPARK_CONFIG} \
  ${FINK_HOME}/bin/$SURVEY/active_learning_loop.py ${HELP_ON_SERVICE} \
  -agg_data_prefix ${AGG_DATA_PREFIX} \
  -night ${NIGHT} \
  -log_level ${LOG_LEVEL} ${EXIT_AFTER}
elif [[ $service == "hostless" ]]; then
  spark-submit --master ${SPARK_MASTER} \
  --packages ${FINK_PACKAGES} \
  --jars ${FINK_JARS} ${PYTHON_EXTRA_FILE} ${EXTRA_SPARK_CONFIG} \
  ${FINK_HOME}/bin/$SURVEY/hostless_detection.py ${HELP_ON_SERVICE} \
  -agg_data_prefix ${AGG_DATA_PREFIX} \
  -night ${NIGHT} \
  -hostless_folder "${FINK_HOME}/hostless_ids" \
  -log_level ${LOG_LEVEL} ${EXIT_AFTER}
elif [[ $service == "dwarf_agn" ]]; then
  spark-submit --master ${SPARK_MASTER} \
  --packages ${FINK_PACKAGES} \
  --jars ${FINK_JARS} ${PYTHON_EXTRA_FILE} ${EXTRA_SPARK_CONFIG} \
  ${FINK_HOME}/bin/$SURVEY/dwarf_agn.py ${HELP_ON_SERVICE} \
  -agg_data_prefix ${AGG_DATA_PREFIX} \
  -night ${NIGHT} \
  -log_level ${LOG_LEVEL} ${EXIT_AFTER}
elif [[ $service == "known_tde" ]]; then
  spark-submit --master ${SPARK_MASTER} \
  --packages ${FINK_PACKAGES} \
  --jars ${FINK_JARS} ${PYTHON_EXTRA_FILE} ${EXTRA_SPARK_CONFIG} \
  ${FINK_HOME}/bin/$SURVEY/known_tde.py ${HELP_ON_SERVICE} \
  -agg_data_prefix ${AGG_DATA_PREFIX} \
  -night ${NIGHT} \
  -log_level ${LOG_LEVEL} ${EXIT_AFTER}
elif [[ $service == "save_schema" ]]; then
  spark-submit --master ${SPARK_MASTER} \
  --packages ${FINK_PACKAGES} \
  --jars ${FINK_JARS} ${PYTHON_EXTRA_FILE} ${EXTRA_SPARK_CONFIG} \
  ${FINK_HOME}/bin/$SURVEY/save_distribution_schema.py ${HELP_ON_SERVICE} ${EXIT_AFTER} \
  -agg_data_prefix ${AGG_DATA_PREFIX} \
  -night ${NIGHT} -log_level ${LOG_LEVEL}
elif [[ $service == "tns_resolver" ]]; then
  spark-submit --master ${SPARK_MASTER} \
  --packages ${FINK_PACKAGES} \
  --jars ${FINK_JARS} ${PYTHON_EXTRA_FILE} ${EXTRA_SPARK_CONFIG} \
  ${FINK_HOME}/bin/$SURVEY/download_tns.py ${HELP_ON_SERVICE} \
  -science_db_name ${SCIENCE_DB_NAME} \
  -science_db_catalogs ${SCIENCE_DB_CATALOGS} \
  -night ${NIGHT} \
  -tns_folder ${TNS_FOLDER} \
  -tns_raw_output ${TNS_RAW_OUTPUT} \
  -log_level ${LOG_LEVEL} ${EXIT_AFTER}
else
  # In case you give an unknown service
  echo "unknown service: $service" >&2
  echo $message_service
  exit 1
fi
