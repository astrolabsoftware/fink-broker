#!/bin/bash
# Copyright 2019 AstroLab Software
# Author: Julien Peloton
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
set -e

message_service="Available services are: checkstream, stream2raw, raw2science, distribution, science_archival, check_science_portal"
message_conf="Typical configuration would be $PWD/conf/fink.conf"
message_help="""
Handle Kafka stream received by Apache Spark\n\n
Usage:\n
    \tto init paths     : fink init [-h] [-c <conf>]\n
    \tto start a service: fink start <service> [-h] [-c <conf>] [--simulator]\n
    \tto stop a service : fink stop  <service> [-h] [-c <conf>]\n
    \tto show services  : fink show [-h] \n\n

To get this help: \n
\tfink \n\n

To get help for a service: \n
\tfink start <service> -h\n\n

$message_service\n
$message_conf\n\n

To see the running processes: \n
\tfink show \n\n

To initialise data folders and paths: \n
\tfink init \n\n

To stop a service or all running processes: \n
\tfink stop <service or all> \n
"""
# Show help if no arguments is given
if [[ $1 == "" ]]; then
  echo -e $message_help
  exit 1
fi

# Grab the command line arguments
while [ "$#" -gt 0 ]; do
  case "$1" in
    "start"|"stop")
        MODE="$1"
        if [[ $2 == "" || $2 == "-c" ]]; then
          echo "$1 requires an argument. ${message_service}" >&2
          exit 1
        fi
        service="$2"
        shift 2
        ;;
    "show")
        nservice=$(ps ax | grep -i 'fink start' | grep -v grep | wc -l)
        echo -e "$nservice Fink service(s) running: "
        ps aux | head -1
        ps aux | grep -i 'fink start' | grep -v grep
        echo "Use <fink stop service_name> to stop a service."
        echo "Use <fink start dashboard> to start the dashboard or check its status."
        exit
        ;;
    "init")
        INITPATH=true
        shift 1
        ;;
    -h)
        HELP_ON_SERVICE="-h"
        shift 1
        ;;
    -c)
        if [[ $2 == "" || $2 == "-s" ]]; then
          echo "$1 requires an argument. ${message_conf}" >&2
          exit 1
        fi
        conf="$2"
        shift 2
        ;;
    --conf=*)
        conf="${1#*=}"
        shift 1
        ;;
    --conf)
        echo "$1 requires an argument" >&2
        exit 1
        ;;
    --simulator)
        SIM_ONLY=true
        shift 1
        ;;
    --version)
        python3 -c "import fink_broker; print(fink_broker.__version__)"
        exit
        ;;
    --night)
      NIGHT="$2"
      shift 2
      ;;
    --exit_after)
        EXIT_AFTER="-exit_after $2"
        shift 2
        ;;
    -*)
        echo "unknown option: $1" >&2
        exit 1
        ;;
    *)
        echo "unknown argument: $1" >&2
        exit 1
        ;;
  esac
done

# Check if the conf file exists
if [[ -f $conf ]]; then
  echo "Reading custom Fink configuration file from " $conf
  source $conf
else
  echo "Reading default Fink conf from " ${FINK_HOME}/conf/fink.conf
  source ${FINK_HOME}/conf/fink.conf
fi

# Initialise paths to store data and checkpoints
if [[ "$INITPATH" = true ]] ; then
  if [[ "$FS_KIND" = "local" ]] ; then
    finkdir='mkdir'
  elif [[ "$FS_KIND" = "hdfs" ]] ; then
    finkdir='hdfs dfs -mkdir'
  else
    echo "$FS_KIND file system not understood! Currently available: FS_KIND={local, hdfs}"
    exit 1
  fi
  $finkdir $DATA_PREFIX
  $finkdir $FINK_ALERT_PATH
  $finkdir $FINK_ALERT_PATH_SCI_TMP
  $finkdir $FINK_ALERT_CHECKPOINT_RAW
  $finkdir $FINK_ALERT_CHECKPOINT_SCI_TMP
  $finkdir $FINK_ALERT_CHECKPOINT_SCI
  echo "Data will be stored under $DATA_PREFIX"
  exit
fi

if [[ "$SIM_ONLY" = true ]] ; then
  KAFKA_IPPORT=$KAFKA_IPPORT_SIM
fi

# Stop services
if [[ $MODE == "stop" ]]; then
  if [[ $service == "all" ]]; then
    SIGNAL=${SIGNAL:-TERM}

    # Kill instance of HBase connector running if needed
    PIDS=$(ps ax | grep -i 'fink' | grep "hbase" | awk '{print $1}')

    if [ -z "$PIDS" ]; then
      echo "No HBase connection to stop"
    else
      echo "Stopping HBase connection..."
      kill -s $SIGNAL $PIDS
    fi

    # Kill other services
    PIDS=$(ps ax | grep -i 'fink' | awk '{print $1}')
    if [ -z "$PIDS" ]; then
      echo "No fink service(s) to stop"
    else
      echo "Stopping all services..."
      kill -s $SIGNAL $PIDS
    fi
  elif [[ $service == "dashboard" ]]; then
    # PID=$(lsof -t -i :${FINKUIPORT})
    FINK_HOME=${FINK_HOME} FINK_UI_PORT=${FINK_UI_PORT} docker-compose -p dashboardnet -f ${FINK_HOME}/docker/docker-compose-ui.yml down
  else
    SIGNAL=${SIGNAL:-TERM}

    # Kill instance of HBase connector running if needed
    if [[ $service == "raw2science" ]]; then
      PIDS=$(ps ax | grep -i 'fink' | grep "hbase" | awk '{print $1}')

      if [ -z "$PIDS" ]; then
        echo "No fink $service service to stop"
      else
        echo "Stopping $service (HBase connection)..."
        kill -s $SIGNAL $PIDS
      fi
    fi

    PIDS=$(ps ax | grep -i 'fink' | grep $service | awk '{print $1}')

    if [ -z "$PIDS" ]; then
      echo "No fink $service service to stop"
    else
      echo "Stopping $service..."
      kill -s $SIGNAL $PIDS
    fi
  fi
  exit 1
fi

# Start services
mkdir -p ${FINK_HOME}/web/data

# Grab Fink and Python version numbers
FINK_VERSION=`fink --version`
PYTHON_VERSION=`python -c "import platform; print(platform.python_version()[:3])"`

# Package fink_broker & userfilters modules
PYTHON_EXTRA_FILE=""
if [[ $DEPLOY_FINK_PYTHON == "true" ]]; then
  cd $FINK_HOME
  python3 setup.py bdist_egg
  PYTHON_EXTRA_FILE="--py-files ${FINK_HOME}/dist/fink_broker-${FINK_VERSION}-py${PYTHON_VERSION}.egg"
  echo "Distributing ${PYTHON_EXTRA_FILE}"
  cd -
fi

if [[ $service == "dashboard" ]]; then
  # Launch the UI
  export is_docker=`command -v docker-compose`
  if [[ -f $is_docker ]]; then
    FINK_HOME=${FINK_HOME} FINK_UI_PORT=${FINK_UI_PORT} docker-compose -p dashboardnet -f ${FINK_HOME}/docker/docker-compose-ui.yml up -d
  else
    echo "docker-compose not found, using python -m http.server"
    echo "This is a workaround and work only from the repo root."
    cd ${FINK_HOME}/web
    python -m http.server ${FINK_UI_PORT}
    cd -
  fi
  echo "Dashboard served at http://localhost:${FINK_UI_PORT}"
elif [[ $service == "checkstream" ]]; then
  # Monitor the stream of alerts
  spark-submit --master ${SPARK_MASTER} \
    --packages ${FINK_PACKAGES} ${PYTHON_EXTRA_FILE} \
    ${SECURED_KAFKA_CONFIG} ${EXTRA_SPARK_CONFIG} \
    ${FINK_HOME}/bin/checkstream.py ${HELP_ON_SERVICE} -servers ${KAFKA_IPPORT} \
    -topic ${KAFKA_TOPIC} -startingoffsets_stream ${KAFKA_STARTING_OFFSET} \
    -log_level ${LOG_LEVEL} ${EXIT_AFTER}
elif [[ $service == "stream2raw" ]]; then

  # Store the stream of alerts
  spark-submit --master ${SPARK_MASTER} \
    --packages ${FINK_PACKAGES} ${PYTHON_EXTRA_FILE} \
    ${SECURED_KAFKA_CONFIG} ${EXTRA_SPARK_CONFIG} \
    ${FINK_HOME}/bin/stream2raw.py ${HELP_ON_SERVICE} -servers ${KAFKA_IPPORT} -topic ${KAFKA_TOPIC} \
    -schema ${FINK_ALERT_SCHEMA} -startingoffsets_stream ${KAFKA_STARTING_OFFSET} \
    -rawdatapath ${FINK_ALERT_PATH} -checkpointpath_raw ${FINK_ALERT_CHECKPOINT_RAW} \
    -tinterval ${FINK_TRIGGER_UPDATE} -log_level ${LOG_LEVEL} ${EXIT_AFTER}
elif [[ $service == "raw2science" ]]; then

  # Store the stream of alerts
  spark-submit --master ${SPARK_MASTER} \
    --packages ${FINK_PACKAGES} \
    --jars ${FINK_JARS} \
    ${PYTHON_EXTRA_FILE} \
    ${SECURED_KAFKA_CONFIG} ${EXTRA_SPARK_CONFIG} \
    ${FINK_HOME}/bin/raw2science.py ${HELP_ON_SERVICE} \
    -rawdatapath ${FINK_ALERT_PATH} \
    -scitmpdatapath ${FINK_ALERT_PATH_SCI_TMP} \
    -checkpointpath_sci_tmp ${FINK_ALERT_CHECKPOINT_SCI_TMP} \
    -log_level ${LOG_LEVEL} ${EXIT_AFTER}
elif [[ $service == "distribution" ]]; then
  # Read configuration for redistribution
  source ${FINK_HOME}/conf/fink.conf.distribution
  # Start the Spark Producer
  spark-submit --master ${SPARK_MASTER} \
  --packages ${FINK_PACKAGES} \
  --jars ${FINK_JARS} ${PYTHON_EXTRA_FILE} ${EXTRA_SPARK_CONFIG} \
  --files ${FINK_HOME}/conf/fink_kafka_producer_jaas.conf \
  --driver-java-options "-Djava.security.auth.login.config=${FINK_PRODUCER_JAAS}" \
  --conf "spark.driver.extraJavaOptions=-Djava.security.auth.login.config=${FINK_PRODUCER_JAAS}" \
  --conf "spark.executor.extraJavaOptions=-Djava.security.auth.login.config=${FINK_PRODUCER_JAAS}" \
  ${FINK_HOME}/bin/distribute.py ${HELP_ON_SERVICE} \
  -scitmpdatapath ${FINK_ALERT_PATH_SCI_TMP} \
  -checkpointpath_kafka ${FINK_ALERT_CHECKPOINT_KAFKA} \
  -distribution_servers ${DISTRIBUTION_SERVERS} \
  -distribution_topic ${DISTRIBUTION_TOPIC} \
  -distribution_schema ${DISTRIBUTION_SCHEMA} \
  -distribution_rules_xml "${DISTRIBUTION_RULES_XML}" \
  -log_level ${LOG_LEVEL} ${EXIT_AFTER}
elif [[ $service == "merge_and_clean" ]]; then
  spark-submit --master ${SPARK_MASTER} \
  --packages ${FINK_PACKAGES} \
  --jars ${FINK_JARS} ${PYTHON_EXTRA_FILE} ${EXTRA_SPARK_CONFIG} \
  ${FINK_HOME}/bin/merge_ztf_night.py ${HELP_ON_SERVICE} \
  -datapath ${DATA_PREFIX} \
  -rawdatapath ${FINK_ALERT_PATH} \
  -scitmpdatapath ${FINK_ALERT_PATH_SCI_TMP} \
  -night ${NIGHT} \
  -fs ${FS_KIND} \
  -log_level ${LOG_LEVEL} ${EXIT_AFTER}
elif [[ $service == "reprocess_night" ]]; then
  spark-submit --master ${SPARK_MASTER} \
  --packages ${FINK_PACKAGES} \
  --jars ${FINK_JARS} ${PYTHON_EXTRA_FILE} ${EXTRA_SPARK_CONFIG} \
  ${FINK_HOME}/bin/raw2science_batch.py ${HELP_ON_SERVICE} \
  -night ${NIGHT} \
  -log_level ${LOG_LEVEL}
elif [[ $service == "science_archival" ]]; then
  # Read configuration for hbase
  source ${FINK_HOME}/conf/fink.conf.hbase
  # Check if the use wants to just drop the catalog on disk
  if [[ "$SAVE_SCIENCE_DB_CATALOG_ONLY" = true ]] ; then
    SAVE_SCIENCE_DB_CATALOG_ONLY="--save_science_db_catalog_only"
  else
    SAVE_SCIENCE_DB_CATALOG_ONLY=""
  fi

  spark-submit --master ${SPARK_MASTER} \
  --packages ${FINK_PACKAGES} \
  --jars ${FINK_JARS} ${PYTHON_EXTRA_FILE} ${EXTRA_SPARK_CONFIG} \
  ${FINK_HOME}/bin/science_archival.py ${HELP_ON_SERVICE} \
  -science_db_name ${SCIENCE_DB_NAME} \
  -science_db_catalog ${SCIENCE_DB_CATALOG} \
  ${SAVE_SCIENCE_DB_CATALOG_ONLY} \
  -night ${NIGHT} \
  -log_level ${LOG_LEVEL} ${EXIT_AFTER}
elif [[ $service == "index_archival" ]]; then
  # Read configuration for hbase
  source ${FINK_HOME}/conf/fink.conf.hbase
  spark-submit --master ${SPARK_MASTER} \
  --packages ${FINK_PACKAGES} \
  --jars ${FINK_JARS} ${PYTHON_EXTRA_FILE} ${EXTRA_SPARK_CONFIG} \
  ${FINK_HOME}/bin/index_archival.py ${HELP_ON_SERVICE} \
  -science_db_name ${SCIENCE_DB_NAME} \
  -science_db_catalog ${SCIENCE_DB_CATALOG} \
  ${SAVE_SCIENCE_DB_CATALOG_ONLY} \
  -night ${NIGHT} \
  -log_level ${LOG_LEVEL} ${EXIT_AFTER}
elif [[ $service == "check_science_portal" ]]; then
  # Read configuration for hbase
  source ${FINK_HOME}/conf/fink.conf.hbase

  spark-submit --master ${SPARK_MASTER} \
  --packages ${FINK_PACKAGES} \
  --jars ${FINK_JARS} ${PYTHON_EXTRA_FILE} ${EXTRA_SPARK_CONFIG} \
  ${FINK_HOME}/bin/access_science_db.py ${HELP_ON_SERVICE} \
  -science_db_catalog ${SCIENCE_DB_CATALOG} \
  -log_level ${LOG_LEVEL} ${EXIT_AFTER}
elif [[ $service == "distribution_test" ]]; then
  # Read configuration for redistribution
  source ${FINK_HOME}/conf/fink.conf.distribution
  # Start a spark consumer to test the distribution
  spark-submit --master ${SPARK_MASTER} \
  --packages ${FINK_PACKAGES} \
  --jars ${FINK_JARS} ${PYTHON_EXTRA_FILE} \
  --files ${FINK_HOME}/conf/fink_kafka_consumer_jaas.conf \
  --driver-java-options "-Djava.security.auth.login.config=${FINK_TEST_CONSUMER_JAAS}" \
  --conf "spark.driver.extraJavaOptions=-Djava.security.auth.login.config=${FINK_TEST_CONSUMER_JAAS}" \
  --conf "spark.executor.extraJavaOptions=-Djava.security.auth.login.config=${FINK_TEST_CONSUMER_JAAS}" \
  ${FINK_HOME}/bin/distribution_test.py ${HELP_ON_SERVICE} ${EXIT_AFTER} \
  -distribution_servers ${DISTRIBUTION_SERVERS} \
  -distribution_topic ${DISTRIBUTION_TOPIC} \
  -distribution_schema ${DISTRIBUTION_SCHEMA} -log_level ${LOG_LEVEL}
else
  # In case you give an unknown service
  echo "unknown service: $service" >&2
  echo $message_service
  exit 1
fi
