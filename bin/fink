#!/bin/bash
# Copyright 2019 AstroLab Software
# Author: Julien Peloton
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
set -e

message_service="Available services are: dashboard, checkstream, stream2raw, raw2science, distribution"
message_conf="Typical configuration would be $PWD/conf/fink.conf"
message_help="""
Handle Kafka stream received by Apache Spark\n\n
Usage:\n
    \tto start: fink start <service> [-h] [-c <conf>] [--simulator]\n
    \tto stop : fink stop  <service> [-h] [-c <conf>]\n\n

To get this help: \n
\tfink \n\n

To get help for a service: \n
\tfink start <service> -h\n\n

$message_service\n
$message_conf\n\n

To see the running processes: \n
\tfink show \n\n

To stop a service or all running processes: \n
\tfink stop <service or all> \n
"""
# Show help if no arguments is given
if [[ $1 == "" ]]; then
  echo -e $message_help
  exit 1
fi

# Grab the command line arguments
while [ "$#" -gt 0 ]; do
  case "$1" in
    "start"|"stop")
        MODE="$1"
        if [[ $2 == "" || $2 == "-c" ]]; then
          echo "$1 requires an argument. ${message_service}" >&2
          exit 1
        fi
        service="$2"
        shift 2
        ;;
    "show")
        nservice=$(ps ax | grep -i 'fink start' | grep -v grep | wc -l)
        echo -e "$nservice Fink service(s) running: "
        ps aux | head -1
        ps aux | grep -i 'fink start' | grep -v grep
        echo "Use <fink stop service_name> to stop a service."
        echo "Use <fink start dashboard> to start the dashboard or check its status."
        exit
        ;;
    -h)
        HELP_ON_SERVICE="-h"
        shift 1
        ;;
    -c)
        if [[ $2 == "" || $2 == "-s" ]]; then
          echo "$1 requires an argument. ${message_conf}" >&2
          exit 1
        fi
        conf="$2"
        shift 2
        ;;
    --conf=*)
        conf="${1#*=}"
        shift 1
        ;;
    --conf)
        echo "$1 requires an argument" >&2
        exit 1
        ;;
    --simulator)
        SIM_ONLY=true
        shift 1
        ;;
    --version)
        python3 -c "import fink_broker; print(fink_broker.__version__)"
        exit
        ;;
    --exit_after)
        EXIT_AFTER="-exit_after $2"
        shift 2
        ;;
    -*)
        echo "unknown option: $1" >&2
        exit 1
        ;;
    *)
        echo "unknown argument: $1" >&2
        exit 1
        ;;
  esac
done

# Check if the conf file exists
if [[ -f $conf ]]; then
  echo "Reading custom Fink configuration file from " $conf
  source $conf
else
  echo "Reading default Fink conf from " ${FINK_HOME}/conf/fink.conf
  source ${FINK_HOME}/conf/fink.conf
fi

if [[ "$SIM_ONLY" = true ]] ; then
  KAFKA_IPPORT=$KAFKA_IPPORT_SIM
  KAFKA_TOPIC=$KAFKA_TOPIC_SIM
fi

# Stop services
if [[ $MODE == "stop" ]]; then
  if [[ $service == "all" ]]; then
    SIGNAL=${SIGNAL:-TERM}

    # Kill instance of HBase connector running if needed
    PIDS=$(ps ax | grep -i 'fink' | grep "hbase" | awk '{print $1}')

    if [ -z "$PIDS" ]; then
      echo "No HBase connection to stop"
    else
      echo "Stopping HBase connection..."
      kill -s $SIGNAL $PIDS
    fi

    # Kill other services
    PIDS=$(ps ax | grep -i 'fink' | awk '{print $1}')
    if [ -z "$PIDS" ]; then
      echo "No fink service(s) to stop"
    else
      echo "Stopping all services..."
      kill -s $SIGNAL $PIDS
    fi
  elif [[ $service == "dashboard" ]]; then
    # PID=$(lsof -t -i :${FINKUIPORT})
    FINK_HOME=${FINK_HOME} FINK_UI_PORT=${FINK_UI_PORT} docker-compose -p dashboardnet -f ${FINK_HOME}/docker/docker-compose-ui.yml down
  elif [[ $service == "simulator" ]]; then
    KAFKA_PORT_SIM=${KAFKA_PORT_SIM} docker-compose -p kafkanet -f ${FINK_HOME}/docker/docker-compose-kafka.yml down
  else
    SIGNAL=${SIGNAL:-TERM}

    # Kill instance of HBase connector running if needed
    if [[ $service == "raw2science" ]]; then
      PIDS=$(ps ax | grep -i 'fink' | grep "hbase" | awk '{print $1}')

      if [ -z "$PIDS" ]; then
        echo "No fink $service service to stop"
      else
        echo "Stopping $service (HBase connection)..."
        kill -s $SIGNAL $PIDS
      fi
    fi

    PIDS=$(ps ax | grep -i 'fink' | grep $service | awk '{print $1}')

    if [ -z "$PIDS" ]; then
      echo "No fink $service service to stop"
    else
      echo "Stopping $service..."
      kill -s $SIGNAL $PIDS
    fi
  fi
  exit 1
fi

# Start services
mkdir -p ${FINK_HOME}/web/data
if [[ $service == "dashboard" ]]; then
  # Launch the UI
  export is_docker=`command -v docker-compose`
  if [[ -f $is_docker ]]; then
    FINK_HOME=${FINK_HOME} FINK_UI_PORT=${FINK_UI_PORT} docker-compose -p dashboardnet -f ${FINK_HOME}/docker/docker-compose-ui.yml up -d
  else
    echo "docker-compose not found, using python -m http.server"
    echo "This is a workaround and work only from the repo root."
    cd ${FINK_HOME}/web
    python -m http.server ${FINK_UI_PORT}
    cd -
  fi
  echo "Dashboard served at http://localhost:${FINK_UI_PORT}"
elif [[ $service == "simulator" ]]; then
  # Launch the simulator - kafka & zookeeper
  export is_docker=`command -v docker-compose`
  if [[ -f $is_docker ]]; then
    KAFKA_PORT_SIM=${KAFKA_PORT_SIM} docker-compose -p kafkanet -f ${FINK_HOME}/docker/docker-compose-kafka.yml up -d
  else
    echo "docker-compose not found"
    exit
  fi
  python3 ${FINK_HOME}/bin/simulate_stream.py \
    -servers ${KAFKA_IPPORT_SIM} -topic ${KAFKA_TOPIC_SIM} -datasimpath ${FINK_DATA_SIM}\
    -tinterval_kafka ${TIME_INTERVAL} -poolsize ${POOLSIZE} ${HELP_ON_SERVICE}
elif [[ $service == "checkstream" ]]; then
  # Monitor the stream of alerts
  spark-submit --master ${SPARK_MASTER} \
    --packages ${FINK_PACKAGES} \
    ${SECURED_KAFKA_CONFIG} ${EXTRA_SPARK_CONFIG} \
    ${FINK_HOME}/bin/checkstream.py -servers ${KAFKA_IPPORT} -topic ${KAFKA_TOPIC} \
    -startingoffsets_stream ${KAFKA_STARTING_OFFSET} -finkwebpath ${FINK_UI_PATH} \
    ${EXIT_AFTER} ${HELP_ON_SERVICE}
elif [[ $service == "stream2raw" ]]; then

  # Store the stream of alerts
  spark-submit --master ${SPARK_MASTER} \
    --packages ${FINK_PACKAGES} \
    ${SECURED_KAFKA_CONFIG} ${EXTRA_SPARK_CONFIG} \
    ${FINK_HOME}/bin/stream2raw.py -servers ${KAFKA_IPPORT} -topic ${KAFKA_TOPIC} \
    -schema ${FINK_ALERT_SCHEMA} -startingoffsets_stream ${KAFKA_STARTING_OFFSET} \
    -rawdatapath ${FINK_ALERT_PATH} -checkpointpath_raw ${FINK_ALERT_CHECKPOINT_RAW} \
    -finkwebpath ${FINK_UI_PATH} -tinterval ${FINK_TRIGGER_UPDATE} \
    ${EXIT_AFTER} ${HELP_ON_SERVICE}
elif [[ $service == "raw2science" ]]; then

  # For use on cluster, the HBase configuration file is actually mandatory.
  if [[ -f ${HBASE_XML_CONF} ]]; then
    export SPARK_CLASSPATH=${HBASE_XML_CONF}
  else
    echo "[WARN] HBase configuration file cannot be found at" ${HBASE_XML_CONF}
    echo "[WARN] Default HBase file will be used " ${FINK_HOME}/conf/hbase-site.xml
    HBASE_XML_CONF=${FINK_HOME}/conf/hbase-site.xml
  fi

  # Store the stream of alerts
  spark-submit --master ${SPARK_MASTER} \
    --packages ${FINK_PACKAGES} \
    --jars ${FINK_JARS} \
    ${SECURED_KAFKA_CONFIG} ${EXTRA_SPARK_CONFIG} \
    --files ${HBASE_XML_CONF} \
    ${FINK_HOME}/bin/raw2science.py -rawdatapath ${FINK_ALERT_PATH} \
    -checkpointpath_sci ${FINK_ALERT_CHECKPOINT_SCI} \
    -science_db_name ${SCIENCE_DB_NAME} \
    -science_db_catalog ${SCIENCE_DB_CATALOG} ${EXIT_AFTER} ${HELP_ON_SERVICE}
elif [[ $service == "distribution" ]]; then
  # Start the Spark Producer
  spark-submit --master ${SPARK_MASTER} \
  --packages ${FINK_PACKAGES} \
  --jars ${FINK_JARS} \
  ${FINK_HOME}/bin/distribute.py -science_db_catalog ${SCIENCE_DB_CATALOG} \
  -science_db_name ${SCIENCE_DB_NAME} \
  -distribution_schema ${DISTRIBUTION_SCHEMA} \
  -startingOffset_dist ${DISTRIBUTION_OFFSET} \
  -checkpointpath_dist ${DISTRIBUTION_OFFSET_FILE} \
  -distribution_rules_xml ${DISTRIBUTION_RULES_XML} ${EXIT_AFTER}
elif [[ $service == "distribution_test" ]]; then
  # Start a spark consumer to test the distribution
  spark-submit --master ${SPARK_MASTER} \
  --packages ${FINK_PACKAGES} \
  --jars ${FINK_JARS} \
  ${FINK_HOME}/bin/distribution_test.py ${EXIT_AFTER} \
  -distribution_schema ${DISTRIBUTION_SCHEMA}
else
  # In case you give an unknown service
  echo "unknown service: $service" >&2
  echo $message_service
  exit 1
fi
