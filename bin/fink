#!/bin/bash
# Copyright 2019-2025 AstroLab Software
# Author: Julien Peloton
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
set -e

# Show help if no arguments is given
if [[ $1 == "" ]]; then
  HELP_ON_SERVICE="-h"
  SURVEY="ztf"
fi

# Grab the command line arguments
while [ "$#" -gt 0 ]; do
  case "$1" in
    "start"|"stop")
      MODE="$1"
      if [[ $2 == "" ]]; then
        echo "$1 requires an argument. ${message_service}" >&2
        exit 1
      fi
      service="$2"
      shift 2
      ;;
    "show")
      nservice=$(ps ax | grep -i 'fink start' | grep -v grep | wc -l)
      echo -e "$nservice Fink service(s) running: "
      ps aux | head -1
      ps aux | grep -i 'fink start' | grep -v grep
      ps aux | grep -i 'gcn_stream start' | grep -v grep
      echo "Use <fink stop service_name> to stop a service."
      exit
      ;;
    -h)
      HELP_ON_SERVICE="-h"
      shift 1
      ;;
    -s)
      SURVEY=$2
      shift 2
      ;;
    -c)
      conf="$2"
      shift 2
      ;;
    -night)
      NIGHT="$2"
      shift 2
      ;;
    -topic)
      KAFKA_TOPIC="$2"
      shift 2
      ;;
    -index_table)
      INDEXTABLE="$2"
      shift 2
      ;;
    -driver-memory)
      DRIVER_MEMORY="$2"
      shift 2
      ;;
    -executor-memory)
      EXECUTOR_MEMORY="$2"
      shift 2
      ;;
    -spark-cores-max)
      SPARK_CORE_MAX="$2"
      shift 2
      ;;
    -spark-executor-cores)
      SPARK_EXECUTOR_CORES="$2"
      shift 2
      ;;
    -conf_distribution)
      conf_distribution="$2"
      shift 2
      ;;
    -exit_after)
      EXIT_AFTER="-exit_after $2"
      shift 2
      ;;
    --simulator)
      SIM_ONLY=true
      shift 1
      ;;
    --version)
      python3 -c "import fink_broker; print(fink_broker.__version__)"
      exit
      ;;
    --noscience)
      NOSCIENCE="--noscience"
      shift 1
      ;;
    -*)
      echo "unknown option: $1" >&2
      exit 1
      ;;
    *)
      echo "unknown argument: $1" >&2
      exit 1
      ;;
  esac
done

__usage="
fink controls the Fink broker real-time and database operations.

  Find more information on Fink at https://fink-broker.org

Usage: $(basename $0) [OPTIONS]

Options:
  -s          Survey name (ztf, rubin). Default is ztf.
  start       Start a service. The list of available services is given at ${FINK_HOME}/bin/${SURVEY}
  stop        Stop a running service.
  show        Show running services.
  -h          Show this help.
  --version   If specified, show the version of the code.

Service options:
  -h                    Show help for the service.
  -c                    Path to configuration file.
  -conf_distribution    Path to the extra configuration file for the distribute service.
  -night                Night to process with format YYYYMMDD
  -topic                Kafka topic name to poll data from.
  -index_table          Name of the index table to populate.
  -driver_memory        Memory to use for the driver (see Spark options)
  -executor_memory      Memory to use for each executor (see Spark options)
  -spark-cores-max      Total number of cores to use for all executors (see Spark options)
  -spark-executor-cores Number of cores per executor (see Spark options)
  -exit_after           Number of seconds after which the service should stop.
  --simulator           If specified, run in simulation mode.
  --noscience           If specified, do not enrich data with science modules.

Examples:
  Start polling for an hour:
  fink start stream2raw -s ztf -night 20190101 \t
	-topic ztf_20190101 -driver_memory 4g -executor_memory 2g \t
	-spark-cores-max 4 -spark-executor-cores 1 -exit_after 3600

  Archive data in HBase:
  fink start archive_science -s ztf -night 20190101 \t
  	-driver-memory 4g -executor-memory 4g \t
	-spark-cores-max 9 -spark-executor-cores 1

Notes: useful wrappers can be found at ${FINK_HOME}/scheduler/${SURVEY}

Scheduler: Typical scheduler at VirtualData is
# Fink real-time
01 00 * * * /home/julien.peloton/fink-broker/scheduler/ztf/launch_stream.sh

# Database service
05 20 * * * /home/julien.peloton/fink-broker/scheduler/ztf/launch_db.sh

# SSOFT - once a month
0 0 1 * * /home/julien.peloton/fink-broker/scheduler/ztf/launch_ssoft.sh
0 12 1 * * /home/julien.peloton/fink-broker/scheduler/ztf/launch_sso_resolver.sh

# Operation reports four times a day
0 07 * * * /home/julien.peloton/fink-broker/scheduler/ztf/check_status.sh --telegram
0 12 * * * /home/julien.peloton/fink-broker/scheduler/ztf/check_status.sh --telegram
0 17 * * * /home/julien.peloton/fink-broker/scheduler/ztf/check_status.sh --telegram
0 22 * * * /home/julien.peloton/fink-broker/scheduler/ztf/check_status.sh --telegram
"

if [[ $SURVEY == "" ]]; then
  echo -e "You need to specify a survey, e.g. fink -s ztf [options]"
  exit 1
fi

if [[ $service == "" ]] && [[ ${HELP_ON_SERVICE} == "-h" ]]; then
  echo -e "$__usage"
  exit 1
fi

# Check service exists
if [[ ! -f ${FINK_HOME}/bin/${SURVEY}/${service}.py ]]; then
  # In case you give an unknown service
  echo "unknown service: $service" >&2
  echo $message_service
  exit 1
fi

# Check if the conf file exists
if [[ -f $conf ]]; then
  echo "Reading custom Fink configuration file from " $conf
  source $conf
else
  echo "Reading default Fink conf from " ${FINK_HOME}/conf/${SURVEY}/fink.conf.prod
  source ${FINK_HOME}/conf/${SURVEY}/fink.conf.prod
fi

if [[ "$SIM_ONLY" = true ]] ; then
  KAFKA_IPPORT=$KAFKA_IPPORT_SIM
fi

# Stop a service
if [[ $MODE == "stop" ]]; then
  if [[ $service == "all" ]]; then
    pkill -f stream2raw
    pkill -f raw2science
    pkill -f distribute
  else
    pkill -f $service
  fi
  exit 1
fi

# Grab Fink and Python version numbers
FINK_VERSION=`fink --version`
PYTHON_VERSION=`python -c "import platform; print(platform.python_version()[:3])"`

# Package fink_broker & userfilters modules
PYTHON_EXTRA_FILE=""
if [[ $DEPLOY_FINK_PYTHON == "true" ]]; then
  cd $FINK_HOME
  python3 setup.py bdist_egg
  PYTHON_EXTRA_FILE="--py-files ${FINK_HOME}/dist/fink_broker-${FINK_VERSION}-py${PYTHON_VERSION}.egg"
  echo "Distributing ${PYTHON_EXTRA_FILE}"
  cd -
fi

# Topic required only for stream2raw
if [[ ${KAFKA_TOPIC} ]]; then
  KAFKA_TOPIC="-topic ${KAFKA_TOPIC}"
else
  KAFKA_TOPIC=""
fi

# Only if the operator pass an index table
if [[ ${INDEXTABLE} ]]; then
  INDEXTABLE="-index_table ${INDEXTABLE}"
else
  INDEXTABLE=""
fi

# Only if the operator pass the location of the TNS catalog
if [[ ${TNS_RAW_OUTPUT} ]]; then
  TNS_RAW_OUTPUT="-tns_raw_output ${TNS_RAW_OUTPUT}"
else
  TNS_RAW_OUTPUT=""
fi

DISTRIBUTE_OPTIONS=""
if [[ $service == "distribute" ]]; then
  if [[ -f $conf_distribution ]]; then
    echo "Reading custom Fink distribution configuration file from " $conf_distribution
    source $conf_distribution
  else
    echo "Reading default Fink distribution conf from " ${FINK_HOME}/conf/${SURVEY}/fink.conf.distribution
    source ${FINK_HOME}/conf/${SURVEY}/fink.conf.distribution
  fi

  DISTRIBUTE_OPTIONS_SPARK="--files ${FINK_HOME}/conf/fink_kafka_producer_jaas.conf --driver-java-options -Djava.security.auth.login.config=${FINK_PRODUCER_JAAS} --conf spark.driver.extraJavaOptions=-Djava.security.auth.login.config=${FINK_PRODUCER_JAAS} --conf spark.executor.extraJavaOptions=-Djava.security.auth.login.config=${FINK_PRODUCER_JAAS} --conf spark.executorEnv.KNWEBHOOK=${KNWEBHOOK} --conf spark.executorEnv.KNWEBHOOK_FINK=${KNWEBHOOK_FINK} --conf spark.executorEnv.KNWEBHOOK_AMA_CL=${KNWEBHOOK_AMA_CL} --conf spark.executorEnv.KNWEBHOOK_AMA_GALAXIES=${KNWEBHOOK_AMA_GALAXIES} --conf spark.executorEnv.KNWEBHOOK_DWF=${KNWEBHOOK_DWF} --conf spark.executorEnv.FINK_TG_TOKEN=${FINK_TG_TOKEN}"
  DISTRIBUTE_OPTIONS="-distribution_servers ${DISTRIBUTION_SERVERS} -substream_prefix ${SUBSTREAM_PREFIX} -kafka_security_protocol ${KAFKA_SECURITY_PROTOCOL} -kafka_sasl_username ${KAFKA_SASL_USERNAME} -kafka_sasl_password ${KAFKA_SASL_PASSWORD} -kafka_buffer_memory ${KAFKA_BUFFER_MEMORY} -kafka_delivery_timeout_ms ${KAFKA_DELIVERY_TIMEOUT_MS}"
fi

spark-submit --master ${SPARK_MASTER} \
  --packages ${FINK_PACKAGES} --jars ${FINK_JARS} ${PYTHON_EXTRA_FILE} \
  ${EXTRA_SPARK_CONFIG} \
  --driver-memory ${DRIVER_MEMORY} --executor-memory ${EXECUTOR_MEMORY} \
  --conf spark.cores.max=${SPARK_CORE_MAX} --conf spark.executor.cores=${SPARK_EXECUTOR_CORES} \
  ${DISTRIBUTE_OPTIONS_SPARK} \
  ${FINK_HOME}/bin/${SURVEY}/${service}.py ${HELP_ON_SERVICE} \
  -producer ${SURVEY} \
  -servers ${KAFKA_IPPORT} \
  -night ${NIGHT} \
  -schema ${FINK_ALERT_SCHEMA} \
  -startingoffsets_stream ${KAFKA_STARTING_OFFSET} \
  -max_offsets_per_trigger ${MAX_OFFSETS_PER_TRIGGER} \
  -online_data_prefix ${ONLINE_DATA_PREFIX} \
  -agg_data_prefix ${AGG_DATA_PREFIX} \
  -tinterval ${FINK_TRIGGER_UPDATE} \
  -mmconfigpath ${FINK_MM_CONFIG} \
  -fink_fat_output ${FINK_FAT_OUTPUT} \
  -hostless_folder ${HOSTLESS_FOLDER} \
  -science_db_name ${SCIENCE_DB_NAME} \
  -science_db_catalogs ${SCIENCE_DB_CATALOGS} \
  -tns_folder ${TNS_FOLDER} \
  ${KAFKA_TOPIC} ${DISTRIBUTE_OPTIONS} ${TNS_RAW_OUTPUT} ${INDEXTABLE} ${NOSCIENCE} \
  -log_level ${LOG_LEVEL} ${EXIT_AFTER}

