#!/bin/bash
# Copyright 2019-2022 AstroLab Software
# Author: Julien Peloton
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
## Script to launch the python test suite and measure the coverage.
## Must be launched as fink_test
set -e
message_help="""
Run the test suite of Fink\n\n
Usage:\n
    \tfink_test [--without-integration] [--without-units] [-h] [-c <conf>]\n\n

By default, both unit tests and integration tests will be run.\n
You can disable the integration tests by specifying --without-integration.\n
You can disable the unit tests by specifying --without-units.\n
You can specify the configuration file followed by -c or else the default (${FINK_HOME}/conf/fink.conf.prod) will be used.\n
Use -h to display this help.
"""

# Grab the command line arguments
NO_INTEGRATION=false
NO_UNITS=false
while [ "$#" -gt 0 ]; do
  case "$1" in
    --without-integration)
        NO_INTEGRATION=true
        shift 1
        ;;
    --without-units)
        NO_UNITS=true
        shift 1
        ;;
    -h)
        echo -e $message_help
        exit
        ;;
    -c)
        if [[ $2 == "" ]]; then
          echo "$1 requires an argument" >&2
          exit 1
        fi
        conf="$2"
        shift 2
        ;;
    --conf=*)
        conf="${1#*=}"
        shift 1
        ;;
    --conf)
        echo "$1 requires an argument" >&2
        exit 1
        ;;
  esac
done

# Source configuration file for tests
if [[ -f $conf ]]; then
  echo "Reading custom test configuration file from " $conf
else
  conf=${FINK_HOME}/conf/fink.conf.prod
  echo "Reading the default test configuration file from " $conf
fi

source $conf

# Export variables for tester.py
export FINK_PACKAGES=$FINK_PACKAGES
export FINK_JARS=$FINK_JARS
export KAFKA_IPPORT_SIM=$KAFKA_IPPORT_SIM
export KAFKA_TOPIC="ztf-stream-sim"

# Add coverage_daemon to the pythonpath. See python/fink_broker/tester.py
export PYTHONPATH="${SPARK_HOME}/python/test_coverage:$PYTHONPATH"
export COVERAGE_PROCESS_START="${FINK_HOME}/.coveragerc"

# Integration tests
if [[ "$NO_INTEGRATION" = false ]] ; then

  # Fire a stream
  fink_simulator -c ${FINK_HOME}/conf/fink_alert_simulator.conf

  # Connect the service to build the raw database from the stream
  fink start stream2raw --simulator --exit_after 60 -c $conf --topic $KAFKA_TOPIC

  # Connect the service to build the science database from the raw one
  fink start raw2science --exit_after 60 -c $conf --night "20190903"

  # Connect to the distribution service
  fink start distribution_test --exit_after 30 -c $conf &

  # Start the distribution service
  fink start distribution --exit_after 30 -c $conf --night "20190903"

  # merge data
  fink start merge -c $conf --night "20190903"

  # Update science portal
  fink start science_archival -c $conf --night 20190903
  fink start index_archival -c $conf --night 20190903 --index_table jd_objectId
  fink start index_archival -c $conf --night 20190903 --index_table pixel128_jd_objectId
  fink start index_archival -c $conf --night 20190903 --index_table pixel4096_jd_objectId
  fink start index_archival -c $conf --night 20190903 --index_table pixel131072_jd_objectId
  fink start index_archival -c $conf --night 20190903 --index_table class_jd_objectId
  fink start index_archival -c $conf --night 20190903 --index_table upper_objectId_jd
  fink start index_archival -c $conf --night 20190903 --index_table uppervalid_objectId_jd
  fink start index_archival -c $conf --night 20190903 --index_table ssnamenr_jd
  fink start index_archival -c $conf --night 20190903 --index_table tracklet_objectId
  # Need API KEY
  # fink start index_archival -c $conf --night 20190903 --index_table tns_jd_objectId

  # SSO candidates
  fink start index_sso_cand_archival -c $conf --night 20190903

  # Object tables
  # fink start object_archival -c $conf --night 20190903

  fink start stats -c $conf --night 20190903

  fink start check_science_portal -c $conf
else
  # Fire another stream
  fink_simulator --docker -c ${FINK_HOME}/conf/fink_alert_simulator.conf

  # Connect the service to build the raw database from the stream
  fink start stream2raw --exit_after 30 --simulator -c $conf --topic $KAFKA_TOPIC

  # Connect the service to build the science database from the raw one
  fink start raw2science --exit_after 40 -c $conf --night "20190903"
fi

if [[ "$NO_UNITS" = false ]] ; then
  # Run the test suite on the modules assuming the integration
  # tests have been run (to build the databases)
  for i in ${FINK_HOME}/fink_broker/*.py
  do
    if [[ ${i##*/} = 'monitoring' ]] ; then
        echo "skip {i}"
    else
      coverage run \
          --source=${FINK_HOME} \
          --rcfile ${FINK_HOME}/.coveragerc $i
    fi
  done
fi

# Combine individual reports in one
coverage combine

unset KAFKA_IPPORT_SIM
unset KAFKA_TOPIC
unset COVERAGE_PROCESS_START
unset FINK_PACKAGES
unset FINK_JARS

coverage report
coverage html
