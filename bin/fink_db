#!/bin/bash
# Copyright 2019-2025 AstroLab Software
# Author: Julien Peloton
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
source ~/.bash_profile

# TODO: add help
# Show help if no arguments is given
if [[ $1 == "" ]]; then
  HELP_ON_SERVICE="-h"
  SURVEY="ztf"
fi

# Grab the command line arguments
while [ "$#" -gt 0 ]; do
  case "$1" in
    -h)
      HELP_ON_SERVICE="-h"
      shift 1
      ;;
    -s)
      SURVEY=$2
      shift 2
      ;;
    -c)
      conf="$2"
      shift 2
      ;;
    -night)
      NIGHT="$2"
      shift 2
      ;;
    -driver-memory)
      DRIVER_MEMORY="$2"
      shift 2
      ;;
    -executor-memory)
      EXECUTOR_MEMORY="$2"
      shift 2
      ;;
    -spark-cores-max)
      SPARK_CORE_MAX="$2"
      shift 2
      ;;
    -spark-executor-cores)
      SPARK_EXECUTOR_CORES="$2"
      shift 2
      ;;
    --merge)
      MERGE=true
      shift 1
      ;;
    --main_table)
      MAIN_TABLE=true
      shift 1
      ;;
    --index_tables)
      INDEX_TABLES=true
      shift 1
      ;;
    --clean_night)
      CLEAN_NIGHT=true
      shift 1
      ;;
    --delete_merge)
      DELETE_MERGE=true
      shift 1
      ;;
    -*)
      echo "unknown option: $1" >&2
      exit 1
      ;;
    *)
      echo "unknown argument: $1" >&2
      exit 1
      ;;
  esac
done

__usage="
fink_db is a wrapper around fink to control the database operations.

Usage: $(basename $0) [OPTIONS]

Options:
  -h                    Show this help.
  -s                    Survey name (ztf, rubin). Default is ztf.
  -c                    Path to configuration file.
  -night                Night to process with format YYYYMMDD
  -driver_memory        Memory to use for the driver (see Spark options)
  -executor_memory      Memory to use for each executor (see Spark options)
  -spark-cores-max      Total number of cores to use for all executors (see Spark options)
  -spark-executor-cores Number of cores per executor (see Spark options)
  --merge               Merge small files from streaming operations from ONLINE_DATA_PREFIX into AGG_DATA_PREFIX (see configuration file)
  --main_table          Push night data to HBase main table
  --index_tables        Push night data to HBase index tables
  --clean_night         Clean temporary files from the night. Should be run once data is merged.
  --delete_merge        Delete merged files in case of error (then restart using --merge).

Notes: another useful wrapper can be found at ${FINK_HOME}/scheduler/${SURVEY}/launch_db.sh

Examples:
fink_db -s ztf --merge                # merge files
fink_db -s ztf --main_table           # Push data to HBase
fink_db -s ztf --index_tables         # Push data to HBase index tables
fink_db -s ztf --clean_night          # Clean temp files for the next night
"

if [[ $service == "" ]] && [[ ${HELP_ON_SERVICE} == "-h" ]]; then
  echo -e "$__usage"
  exit 1
fi

if [[ $SURVEY == "" ]]; then
  echo -e "You need to specify a survey, e.g. fink -s ztf [options]"
  exit 1
fi

if [[ ! $NIGHT ]]; then
  # Current night
  NIGHT=`date +"%Y%m%d"`
fi
echo "Processing night ${NIGHT} for survey ${SURVEY}"

# Check if the conf file exists
if [[ -f $conf ]]; then
  echo "Reading custom Fink configuration file from " $conf
else
  echo "Reading default Fink conf from " ${FINK_HOME}/conf/${SURVEY}/fink.conf.prod
  conf=${FINK_HOME}/conf/${SURVEY}/fink.conf.prod
fi

source $conf

# Merge streaming data
if [[ ${MERGE} == true ]]; then
  $(hdfs dfs -test -d ${ONLINE_DATA_PREFIX}/science/${NIGHT})
  if [[ $? == 0 ]]; then
    echo "merge_and_clean"
    # if nothing specified, get default
    if [[ ! ${DRIVER_MEMORY} ]]; then
      RESOURCES="-driver-memory 4g -executor-memory 16g -spark-cores-max 80 -spark-executor-cores 8"
    else
      RESOURCES="-driver-memory ${DRIVER_MEMORY} -executor-memory ${EXECUTOR_MEMORY} -spark-cores-max ${SPARK_CORE_MAX} -spark-executor-cores ${SPARK_EXECUTOR_CORES}"
    fi
    echo "Using Spark resources: ${RESOURCES}"
    fink start merge -s ${SURVEY} -c ${conf} -night ${NIGHT} ${RESOURCES} > ${FINK_HOME}/broker_logs/merge_${SURVEY}_${NIGHT}.log 2>&1
  else
    echo "No data at ${ONLINE_DATA_PREFIX}/science/${NIGHT}"
    echo "Merging aborted..."
    exit 1
  fi
fi

if [[ ! ${DRIVER_MEMORY} ]]; then
  RESOURCES="-driver-memory 4g -executor-memory 4g -spark-cores-max 9 -spark-executor-cores 1"
else
  RESOURCES="-driver-memory ${DRIVER_MEMORY} -executor-memory ${EXECUTOR_MEMORY} -spark-cores-max ${SPARK_CORE_MAX} -spark-executor-cores ${SPARK_EXECUTOR_CORES}"
fi
YEAR=${NIGHT:0:4}
MONTH=${NIGHT:4:2}
DAY=${NIGHT:6:2}

# Aggregated data is available
HAS_DATA=$(hdfs dfs -test -d ${AGG_DATA_PREFIX}/science/year=${YEAR}/month=${MONTH}/day=${DAY})

if [[ ${MAIN_TABLE} == true ]]; then
  if [[ ${HAS_DATA} == 0 ]]; then
    echo "Using Spark resources: ${RESOURCES}"
    echo "science_archival"
    fink start archive_science -s ${SURVEY} -c ${conf} -night ${NIGHT} ${RESOURCES} > ${FINK_HOME}/broker_logs/archive_science_${SURVEY}_${NIGHT}.log 2>&1
  else
    echo "No data at ${AGG_DATA_PREFIX}/science/year=${YEAR}/month=${MONTH}/day=${DAY}"
    echo "Did you run --merge before?"
    echo "Pushing data to tables aborted..."
  fi
fi

if [[ ${INDEX_TABLES} == true ]]; then
  if [[ ${HAS_DATA} == 0 ]]; then
    echo "Using Spark resources: ${RESOURCES}"

    echo "1/12: Download latest TNS data"
    fink start tns_resolver -s ${SURVEY} -c ${conf} -night ${NIGHT} ${RESOURCES} > ${FINK_HOME}/broker_logs/tns_resolver_${SURVEY}_${NIGHT}.log 2>&1

    echo "2/12: images_archival"
    fink start archive_images -s ${SURVEY} -c ${conf} -night ${NIGHT} ${RESOURCES} > ${FINK_HOME}/broker_logs/archive_images_${SURVEY}_${NIGHT}.log 2>&1

    echo "3/12: Update index tables"
    fink start index_archival -s ${SURVEY} -c ${conf} -night ${NIGHT} ${RESOURCES} --index_table pixel128_jdstarthist_objectId > ${FINK_HOME}/broker_logs/index_pixel128_jd_objectId_${SURVEY}_${NIGHT}.log 2>&1
    fink start index_archival -s ${SURVEY} -c ${conf} -night ${NIGHT} ${RESOURCES} --index_table class_jd_objectId > ${FINK_HOME}/broker_logs/index_class_jd_objectId_${SURVEY}_${NIGHT}.log 2>&1
    fink start index_archival -s ${SURVEY} -c ${conf} -night ${NIGHT} ${RESOURCES} --index_table upper_objectId_jd > ${FINK_HOME}/broker_logs/index_upper_objectId_jd_${SURVEY}_${NIGHT}.log 2>&1
    fink start index_archival -s ${SURVEY} -c ${conf} -night ${NIGHT} ${RESOURCES} --index_table ssnamenr_jd > ${FINK_HOME}/broker_logs/index_ssnamenr_jd_${SURVEY}_${NIGHT}.log 2>&1
    fink start index_archival -s ${SURVEY} -c ${conf} -night ${NIGHT} ${RESOURCES} --index_table uppervalid_objectId_jd > ${FINK_HOME}/broker_logs/index_uppervalid_objectId_jd_${SURVEY}_${NIGHT}.log 2>&1
    fink start index_archival -s ${SURVEY} -c ${conf} -night ${NIGHT} ${RESOURCES} --index_table tracklet_objectId > ${FINK_HOME}/broker_logs/index_tracklet_objectId_${SURVEY}_${NIGHT}.log 2>&1
    fink start index_archival -s ${SURVEY} -c ${conf} -night ${NIGHT} ${RESOURCES} --index_table tns_jd_objectId > ${FINK_HOME}/broker_logs/index_tns_jd_objectId_${SURVEY}_${NIGHT}.log 2>&1

    echo "4/12: Push TNS candidates"
    fink start push_to_tns -s ${SURVEY} -c ${conf} -night ${NIGHT} ${RESOURCES} > ${FINK_HOME}/broker_logs/push_to_tns_${SURVEY}_${NIGHT}.log 2>&1

    echo "5/12: Push Anomaly candidates"
    fink start archive_anomaly -s ${SURVEY} -c ${conf} -night ${NIGHT} ${RESOURCES} > ${FINK_HOME}/broker_logs/archive_anomaly_${SURVEY}_${NIGHT}.log 2>&1

    echo "6/12: Push Active Learning loop candidates"
    fink start archive_ia_active_learning -s ${SURVEY} -c ${conf} -night ${NIGHT} ${RESOURCES} > ${FINK_HOME}/broker_logs/archive_ia_active_learning_${SURVEY}_${NIGHT}.log 2>&1

    echo "7/12: Push Hostless candidates"
    fink start archive_hostless -s ${SURVEY} -c ${conf} -night ${NIGHT} ${RESOURCES} > ${FINK_HOME}/broker_logs/archive_hostless_${SURVEY}_${NIGHT}.log 2>&1

    echo "8/12: Send Dwarf AGN candidates"
    fink start archive_dwarf_agn -s ${SURVEY} -c ${conf} -night ${NIGHT} ${RESOURCES} > ${FINK_HOME}/broker_logs/archive_dwarf_agn_${SURVEY}_${NIGHT}.log 2>&1

    echo "9/12: Send Known TDE candidates"
    fink start archive_known_tde -s ${SURVEY} -c ${conf} -night ${NIGHT} ${RESOURCES} > ${FINK_HOME}/broker_logs/archive_known_tde_${SURVEY}_${NIGHT}.log 2>&1

    echo "10/12: Update statistics"
    fink start archive_statistics -s ${SURVEY} -c ${conf} -night ${NIGHT} ${RESOURCES} > ${FINK_HOME}/broker_logs/archive_statistics_${SURVEY}_${NIGHT}.log 2>&1

    echo "11/12: Call to Fink-Fat"
    fink_fat associations candidates --config $FINK_HOME/conf/fink_fat.conf --night ${YEAR}-${MONTH}-${DAY} --verbose > ${FINK_HOME}/broker_logs/fink_fat_association_${NIGHT}.log 2>&1
    fink_fat solve_orbit candidates local --config $FINK_HOME/conf/fink_fat.conf --verbose > ${FINK_HOME}/broker_logs/fink_fat_solve_orbit_${NIGHT}.log 2>&1

    echo "12/12: Push SSO candidates to HBase"
    fink start archive_sso_cand -s ${SURVEY} -c ${conf} -night ${NIGHT} ${RESOURCES} > ${FINK_HOME}/broker_logs/archive_sso_cand_${SURVEY}_${NIGHT}.log 2>&1
  else
    echo "No data at ${AGG_DATA_PREFIX}/science/year=${YEAR}/month=${MONTH}/day=${DAY}"
    echo "Did you run --merge before?"
    echo "Pushing data to tables aborted..."
  fi
fi

if [[ $DELETE_MERGE == true ]]; then
  echo "Deleting merged files on HDFS..."
  # If merge went wrong -- clean
  hdfs dfs -rm -r ${AGG_DATA_PREFIX}/raw/year=${YEAR}/month=${MONTH}/day=${DAY}
  hdfs dfs -rm -r ${AGG_DATA_PREFIX}/science/year=${YEAR}/month=${MONTH}/day=${DAY}
fi

if [[ ${CLEAN_NIGHT} == true ]]; then
  # Delete temporary files after the night
  $(hdfs dfs -test -d ${AGG_DATA_PREFIX}/science/year=${YEAR}/month=${MONTH}/day=${DAY})
  if [[ $? == 0 ]]; then
    echo "Cleaning temporary files on HDFS..."
    # Remove data path
    hdfs dfs -rm -r ${ONLINE_DATA_PREFIX}/raw/${NIGHT}
    hdfs dfs -rm -r ${ONLINE_DATA_PREFIX}/science/${NIGHT}

    # Remove checkpoints
    hdfs dfs -rm -r ${ONLINE_DATA_PREFIX}/raw_checkpoint/${NIGHT}
    hdfs dfs -rm -r ${ONLINE_DATA_PREFIX}/science_checkpoint/${NIGHT}
    hdfs dfs -rm -r ${ONLINE_DATA_PREFIX}/kafka_checkpoint/${NIGHT}

    # Remove checkpoints for fink-mm
    hdfs dfs -rm -r /user/julien.peloton/fink_mm/gcn_x_ztf/online/_spark_metadata
    hdfs dfs -rm -r /user/julien.peloton/fink_mm/gcn_x_ztf/online_checkpoint
    hdfs dfs -rm -r /user/julien.peloton/fink_mm/gcn_x_ztf/mm_distribute_checkpoint

  else
    echo "No data at ${AGG_DATA_PREFIX}/science/year=${YEAR}/month=${MONTH}/day=${DAY}"
    echo "Cleaning aborted..."
    exit 1
  fi
fi
