#!/bin/bash
# Copyright 2019-2025 AstroLab Software
# Author: Julien Peloton
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
source ~/.bash_profile

# TODO: add help
# Show help if no arguments is given
if [[ $1 == "" ]]; then
  HELP_ON_SERVICE="-h"
  SURVEY="ztf"
fi

# Grab the command line arguments
while [ "$#" -gt 0 ]; do
  case "$1" in
    -h)
      HELP_ON_SERVICE="-h"
      shift 1
      ;;
    -s)
      SURVEY=$2
      shift 2
      ;;
    -c)
      conf="$2"
      shift 2
      ;;
    --merge)
      MERGE=true
      shift 1
      ;;
    --main_table)
      MAIN_TABLE=true
      shift 1
      ;;
    --index_tables)
      INDEX_TABLES=true
      shift 1
      ;;
    --clean_night)
      CLEAN_NIGHT=true
      shift 1
      ;;
    --clean_merge)
      CLEAN_MERGE=true
      shift 1
      ;;
    -night)
      NIGHT="$2"
      shift 2
      ;;
    -driver-memory)
      DRIVER_MEMORY="$2"
      shift 2
      ;;
    -executor-memory)
      EXECUTOR_MEMORY="$2"
      shift 2
      ;;
    -spark-cores-max)
      SPARK_CORE_MAX="$2"
      shift 2
      ;;
    -spark-executor-cores)
      SPARK_EXECUTOR_CORES="$2"
      shift 2
      ;;
    -*)
      echo "unknown option: $1" >&2
      exit 1
      ;;
    *)
      echo "unknown argument: $1" >&2
      exit 1
      ;;
  esac
done

if [[ ! $NIGHT ]]; then
  # Current night
  NIGHT=`date +"%Y%m%d"`
fi
echo "Processing night ${NIGHT} for survey ${SURVEY}"

# Check if the conf file exists
if [[ -f $conf ]]; then
  echo "Reading custom Fink configuration file from " $conf
else
  echo "Reading default Fink conf from " ${FINK_HOME}/conf/${SURVEY}/fink.conf.prod
  conf=${FINK_HOME}/conf/${SURVEY}/fink.conf.prod
fi

# Merge streaming data
if [[ ${MERGE} == true ]]; then
  $(hdfs dfs -test -d /user/julien.peloton/online/science/${NIGHT})
  if [[ $? == 0 ]]; then
    echo "merge_and_clean"
    # if nothing specified, get default
    if [[ ! ${DRIVER_MEMORY} ]]; then
      RESOURCES="-driver-memory 4g -executor-memory 16g -spark-cores-max 80 -spark-executor-cores 8"
    else
      RESOURCES="-driver-memory ${DRIVER_MEMORY} -executor-memory ${EXECUTOR_MEMORY} -spark-cores-max ${SPARK_CORE_MAX} -spark-executor-cores ${SPARK_EXECUTOR_CORES}"
    fi
    fink start merge -s ${SURVEY} -c ${conf} -night ${NIGHT} ${RESOURCES} > ${FINK_HOME}/broker_logs/merge_${SURVEY}_${NIGHT}.log 2>&1
  else
    echo "No data at /user/julien.peloton/online/science/${NIGHT}"
    echo "Merging aborted..."
  fi
fi

if [[ ! ${DRIVER_MEMORY} ]]; then
  RESOURCES="-driver-memory 4g -executor-memory 4g -spark-cores-max 9 -spark-executor-cores 1"
else
  RESOURCES="-driver-memory ${DRIVER_MEMORY} -executor-memory ${EXECUTOR_MEMORY} -spark-cores-max ${SPARK_CORE_MAX} -spark-executor-cores ${SPARK_EXECUTOR_CORES}"
fi
YEAR=${NIGHT:0:4}
MONTH=${NIGHT:4:2}
DAY=${NIGHT:6:2}

# Aggregated data is available
$(hdfs dfs -test -d /user/julien.peloton/archive/science/year=${YEAR}/month=${MONTH}/day=${DAY})
if [[ $? == 0 ]]; then
  if [[ ${MAIN_TABLE} == true ]]; then
    echo "science_archival"
    fink start archive_science -s ${SURVEY} -c ${conf} -night ${NIGHT} ${RESOURCES} > ${FINK_HOME}/broker_logs/archive_science_${SURVEY}_${NIGHT}.log 2>&1
  fi

  if [[ ${INDEX_TABLES} == true ]]; then
    echo "Download latest TNS data"
    fink start tns_resolver -s ${SURVEY} -c ${conf} -night ${NIGHT} ${RESOURCES} > ${FINK_HOME}/broker_logs/tns_resolver_${SURVEY}_${NIGHT}.log 2>&1

    echo "images_archival"
    fink start archive_images -s ${SURVEY} -c ${conf} -night ${NIGHT} ${RESOURCES} > ${FINK_HOME}/broker_logs/archive_images_${SURVEY}_${NIGHT}.log 2>&1

    echo "Update index tables"
    fink start index_archival -s ${SURVEY} -c ${conf} -night ${NIGHT} ${RESOURCES} --index_table pixel128_jdstarthist_objectId > ${FINK_HOME}/broker_logs/index_pixel128_jd_objectId_${SURVEY}_${NIGHT}.log 2>&1
    fink start index_archival -s ${SURVEY} -c ${conf} -night ${NIGHT} ${RESOURCES} --index_table class_jd_objectId > ${FINK_HOME}/broker_logs/index_class_jd_objectId_${SURVEY}_${NIGHT}.log 2>&1
    fink start index_archival -s ${SURVEY} -c ${conf} -night ${NIGHT} ${RESOURCES} --index_table upper_objectId_jd > ${FINK_HOME}/broker_logs/index_upper_objectId_jd_${SURVEY}_${NIGHT}.log 2>&1
    fink start index_archival -s ${SURVEY} -c ${conf} -night ${NIGHT} ${RESOURCES} --index_table ssnamenr_jd > ${FINK_HOME}/broker_logs/index_ssnamenr_jd_${SURVEY}_${NIGHT}.log 2>&1
    fink start index_archival -s ${SURVEY} -c ${conf} -night ${NIGHT} ${RESOURCES} --index_table uppervalid_objectId_jd > ${FINK_HOME}/broker_logs/index_uppervalid_objectId_jd_${SURVEY}_${NIGHT}.log 2>&1
    fink start index_archival -s ${SURVEY} -c ${conf} -night ${NIGHT} ${RESOURCES} --index_table tracklet_objectId > ${FINK_HOME}/broker_logs/index_tracklet_objectId_${SURVEY}_${NIGHT}.log 2>&1
    fink start index_archival -s ${SURVEY} -c ${conf} -night ${NIGHT} ${RESOURCES} --index_table tns_jd_objectId > ${FINK_HOME}/broker_logs/index_tns_jd_objectId_${SURVEY}_${NIGHT}.log 2>&1

    echo "Push TNS candidates"
    fink start push_to_tns -s ${SURVEY} -c ${conf} -night ${NIGHT} ${RESOURCES} > ${FINK_HOME}/broker_logs/push_to_tns_${SURVEY}_${NIGHT}.log 2>&1

    echo "Push Anomaly candidates"
    fink start archive_anomaly -s ${SURVEY} -c ${conf} -night ${NIGHT} ${RESOURCES} > ${FINK_HOME}/broker_logs/archive_anomaly_${SURVEY}_${NIGHT}.log 2>&1

    echo "Push Active Learning loop candidates"
    fink start archive_ia_active_learning -s ${SURVEY} -c ${conf} -night ${NIGHT} ${RESOURCES} > ${FINK_HOME}/broker_logs/archive_ia_active_learning_${SURVEY}_${NIGHT}.log 2>&1

    echo "Push Hostless candidates"
    fink start archive_hostless -s ${SURVEY} -c ${conf} -night ${NIGHT} ${RESOURCES} > ${FINK_HOME}/broker_logs/archive_hostless_${SURVEY}_${NIGHT}.log 2>&1

    echo "Send Dwarf AGN candidates"
    fink start archive_dwarf_agn -s ${SURVEY} -c ${conf} -night ${NIGHT} ${RESOURCES} > ${FINK_HOME}/broker_logs/archive_dwarf_agn_${SURVEY}_${NIGHT}.log 2>&1

    echo "Send Known TDE candidates"
    fink start archive_known_tde -s ${SURVEY} -c ${conf} -night ${NIGHT} ${RESOURCES} > ${FINK_HOME}/broker_logs/archive_known_tde_${SURVEY}_${NIGHT}.log 2>&1

    echo "Update statistics"
    fink start archive_statistics -s ${SURVEY} -c ${conf} -night ${NIGHT} ${RESOURCES} > ${FINK_HOME}/broker_logs/archive_statistics_${SURVEY}_${NIGHT}.log 2>&1

    echo "Call to Fink-Fat"
    fink_fat associations candidates --config $FINK_HOME/conf/fink_fat.conf --night ${YEAR}-${MONTH}-${DAY} --verbose > ${FINK_HOME}/broker_logs/fink_fat_association_${NIGHT}.log 2>&1
    fink_fat solve_orbit candidates local --config $FINK_HOME/conf/fink_fat.conf --verbose > ${FINK_HOME}/broker_logs/fink_fat_solve_orbit_${NIGHT}.log 2>&1

    echo "Push SSO candidates to HBase"
    fink start archive_sso_cand -s ${SURVEY} -c ${conf} -night ${NIGHT} ${RESOURCES} > ${FINK_HOME}/broker_logs/archive_sso_cand_${SURVEY}_${NIGHT}.log 2>&1
  fi
else
    echo "No data at /user/julien.peloton/archive/science/year=${YEAR}/month=${MONTH}/day=${DAY}"
    echo "Did you run --merge before?"
    echo "Pushing data to tables aborted..."
fi

if [[ $CLEAN_MERGE == true ]]; then
  echo "Cleaning merged files..."
  # If merge went wrong -- clean
  hdfs dfs -rm -r /user/julien.peloton/archive/raw/year=${YEAR}/month=${MONTH}/day=${DAY}
  hdfs dfs -rm -r /user/julien.peloton/archive/science/year=${YEAR}/month=${MONTH}/day=${DAY}
fi

if [[ ${CLEAN_NIGHT} == true ]]; then
  # Delete temporary files after the night
  $(hdfs dfs -test -d /user/julien.peloton/archive/science/year=${YEAR}/month=${MONTH}/day=${DAY})
  if [[ $? == 0 ]]; then
    echo "Cleaning temporary files..."
    # Remove data path
    hdfs dfs -rm -r /user/julien.peloton/online/raw/${NIGHT}
    hdfs dfs -rm -r /user/julien.peloton/online/science/${NIGHT}

    # Remove checkpoints
    hdfs dfs -rm -r /user/julien.peloton/online/raw_checkpoint/${NIGHT}
    hdfs dfs -rm -r /user/julien.peloton/online/science_checkpoint/${NIGHT}
    hdfs dfs -rm -r /user/julien.peloton/online/kafka_checkpoint/${NIGHT}

    # Remove checkpoints for fink-mm
    hdfs dfs -rm -r /user/julien.peloton/fink_mm/gcn_x_ztf/online/_spark_metadata
    hdfs dfs -rm -r /user/julien.peloton/fink_mm/gcn_x_ztf/online_checkpoint
    hdfs dfs -rm -r /user/julien.peloton/fink_mm/gcn_x_ztf/mm_distribute_checkpoint

  else
    echo "No data at /user/julien.peloton/archive/science/year=${YEAR}/month=${MONTH}/day=${DAY}"
    echo "Cleaning aborted..."
  fi
fi
