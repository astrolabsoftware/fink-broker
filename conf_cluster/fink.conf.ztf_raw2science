# Copyright 2018-2022 AstroLab Software
# Author: Julien Peloton
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
######################################
# Local mode (Kafka cluster is spun up on-the-fly in docker).
# Must match the ones used for the Producer
KAFKA_PORT_SIM=29092
KAFKA_IPPORT_SIM="localhost:${KAFKA_PORT_SIM}"

# Cluster mode - you need a Kafka cluster installed with write mode.
# Must match the one used for the Producer
KAFKA_IPPORT="public2.alerts.ztf.uw.edu:9094"

# From which offset you want to start pulling data. Options are:
# latest (only new data), earliest (connect from the oldest
# offset available), or a number (see Spark Kafka integration).
KAFKA_STARTING_OFFSET="earliest"

# Apache Spark mode
SPARK_MASTER="mesos://vm-75063.lal.in2p3.fr:5050"

# Should be Spark options actually (cluster resources, ...)
EXTRA_SPARK_CONFIG='--driver-memory 4g --executor-memory 4g --conf spark.cores.max=8 --conf spark.executor.cores=2 '
EXTRA_SPARK_CONFIG+='--conf spark.mesos.principal=lsst --conf spark.mesos.secret=secret --conf spark.mesos.role=lsst '
EXTRA_SPARK_CONFIG+='--conf spark.executorEnv.HOME=/home/julien.peloton '
EXTRA_SPARK_CONFIG+='--repositories https://packages.confluent.io/maven/ '
EXTRA_SPARK_CONFIG+='--files /tmp/jaas.conf '
EXTRA_SPARK_CONFIG+='--driver-java-options -Djava.security.auth.login.config=/tmp/jaas.conf '
echo $EXTRA_SPARK_CONFIG

#EXTRA_SPARK_CONFIG="--driver-memory 15G --executor-memory 1500M --executor-cores 1 --num-executors 68"
#EXTRA_SPARK_CONFIG=""

# Should be Kafka secured options actually (to allow connection to Kafka)
SECURED_KAFKA_CONFIG=''

# These are the Maven Coordinates of dependencies for Fink
# Change the version according to your Spark version.
# NOTE: HBase packages are not required for Parquet archiving
# za.co.absa:abris_2.11:3.1.1,\
#  io.confluent:kafka-avro-serializer:5.3.1,\
#   io.confluent:kafka-schema-registry-client:5.3.1,\
#    io.confluent:common-utils:5.3.1,\
FINK_PACKAGES=\
za.co.absa:abris_2.11:3.1.1,\
org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.4,\
org.apache.spark:spark-avro_2.11:2.4.4,\
org.apache.hbase:hbase-client:2.0.5,\
org.apache.hbase:hbase-common:2.0.5,\
org.apache.hbase:hbase-mapreduce:2.0.5

# Other dependencies (incl. Scala part of Fink)
# ${FINK_HOME}/libs/abris_2.11-3.1.2-SNAPSHOT.jar,\
FINK_JARS=${FINK_HOME}/libs/fink-broker_2.11-0.7.0.jar,\
${FINK_HOME}/libs/shc-core-1.1.3-2.4-s_2.11.jar

# Time interval between 2 trigger updates (second)
# i.e. the timing of streaming data processing.
# If 0, the query will be executed in micro-batch mode,
# where micro-batches will be generated as soon as the previous
# micro-batch has completed processing.
# Note that this timing is also used for updating the dashboard.
FINK_TRIGGER_UPDATE=60

# Alert schema
# Full path to schema to decode the alerts
FINK_ALERT_SCHEMA="${FINK_HOME}/schemas/template_schema_ZTF_3p3.avro"

# Prefix path on disk to save live data.
# They can be in local FS (/path/ or files:///path/) or
# in distributed FS (e.g. hdfs:///path/).
# Be careful though to have enough disk space!
FS_KIND=hdfs
ONLINE_DATA_PREFIX="hdfs://134.158.75.222:8020///user/fink/online"
AGG_DATA_PREFIX="hdfs://134.158.75.222:8020///user/fink/archive"

# The name of the HBase table
SCIENCE_DB_NAME=""
SCIENCE_DB_CATALOGS=${FINK_HOME}/catalogs_hbase

# The name of the HBase table
SCIENCE_DB_NAME=""
SCIENCE_DB_CATALOG=${FINK_HOME}/catalog.json

# HBase configuration file - must be under ${SPARK_HOME}/conf
# You can find an example in ${FINK_HOME}/conf
HBASE_XML_CONF=${SPARK_HOME}/conf/hbase-site.xml

# The minimum level of log for FINK: OFF, DEBUG, INFO, WARN, ERROR, CRITICAL
# Note that for Spark, the level is set to WARN (see log4j.properties)
LOG_LEVEL=INFO

# If set to false, it will assume the fink_broker is in your PYTHONPATH
# otherwise it will package it, and send it to the executors.
DEPLOY_FINK_PYTHON=true

# Path to fink-fat output
FINK_FAT_OUTPUT=

######################################
# Slack
# OAuth Access Token for Slack Workspace
SLACK_API_TOKEN=""
