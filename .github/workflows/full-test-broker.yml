name: Sentinel

on:
  push:
    branches:
      - master
  pull_request:

jobs:
  install-checks:
    runs-on: ubuntu-latest
    steps:
      - name: Check HBase 2.2.7 availability
        run: |
          wget --spider https://archive.apache.org/dist/hbase/2.2.7/hbase-2.2.7-bin.tar.gz
      - name: Check Kafka 2.8.1 availability
        run: |
          wget --spider https://www.apache.org/dist/kafka/2.8.1/kafka_2.12-2.8.1.tgz
      - name: Check Spark 3.1.3 availability
        run: |
          wget --spider http://archive.apache.org/dist/spark/spark-3.1.3/spark-3.1.3-bin-hadoop2.7.tgz
  test-suite:
    needs: install-checks
    runs-on: ubuntu-latest
    container:
      image: quay.io/centos/centos:stream9
    strategy:
      matrix:
        python-version: [3.7]
        spark-version: [3.1.3]
    env:
      SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
    steps:
    - uses: actions/checkout@v2
    - name: Cache Python dependencies
      id: cache-python
      uses: actions/cache@v2
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}-${{ hashFiles('**/install_python_deps.sh') }}
        restore-keys: |
          ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}-${{ hashFiles('**/install_python_deps.sh') }}
    - name: Set up Miniconda ${{ matrix.python-version }}
      uses: conda-incubator/setup-miniconda@v2
      with:
        miniconda-version: "latest"
        auto-activate-base: true
        activate-environment: true
        channels: conda-forge
        python-version: ${{ matrix.python-version }}
    - name: Install add-ons
      run: |
        dnf install -y which git wget java-11-openjdk
        echo "JAVA_HOME=$(dirname $(dirname $(readlink -f $(which java))))" >> $GITHUB_ENV
    - name: Set up env [1/3]
      run: |
        echo "FINK_HOME=$GITHUB_WORKSPACE" >> $GITHUB_ENV
    - name: Install HBase
      run: |
        source conf/install_hbase.sh
    - name: Install Kafka
      run: |
        source conf/install_kafka.sh
        echo "KAFKA_HOME=$FINK_HOME/kafka" >> $GITHUB_ENV
    - name: Install Spark
      run: |
        wget --quiet https://archive.apache.org/dist/spark/spark-${{ matrix.spark-version }}/spark-${{ matrix.spark-version }}-bin-hadoop2.7.tgz
        tar -xf spark-${{ matrix.spark-version }}-bin-hadoop2.7.tgz
        echo "SPARK_HOME=$FINK_HOME/spark-${{ matrix.spark-version }}-bin-hadoop2.7" >> $GITHUB_ENV
    - name: Download test data
      run: |
        cd $FINK_HOME/datasim
        source $FINK_HOME/datasim/download_ztf_alert_data.sh
        cd $FINK_HOME
    - name: Download simulator
      run: |
        git clone https://github.com/astrolabsoftware/fink-alert-simulator.git
        echo "FINK_ALERT_SIMULATOR=${FINK_HOME}/fink-alert-simulator" >> $GITHUB_ENV
    - name: Set up env [2/3]
      run: |
        echo "SPARKLIB=${SPARK_HOME}/python:${SPARK_HOME}/python/lib/py4j-0.10.9-src.zip" >> $GITHUB_ENV
        echo "$HOME/.local/bin:${FINK_HOME}/bin" >> $GITHUB_PATH
        echo "${SPARK_HOME}/bin:${SPARK_HOME}/sbin" >> $GITHUB_PATH
        echo "$FINK_ALERT_SIMULATOR/bin" >> $GITHUB_PATH
        echo "spark.yarn.jars=${SPARK_HOME}/jars/*.jar" > ${SPARK_HOME}/conf/spark-defaults.conf
    - name: Install Python dependencies
      run: |
        echo `python -V`
        echo `which python`
        pip install --upgrade pip setuptools wheel
        source ./install_python_deps.sh --astronet-token ${{ secrets.ASTRONET_TOKEN }}
    - name: Set up env [3/3]
      run: |
        echo "PYTHONPATH="${SPARKLIB}:${FINK_HOME}:${FINK_ALERT_SIMULATOR}"" >> $GITHUB_ENV
    - name: Check env
      run: |
        echo "FINK_HOME: $FINK_HOME"
        echo "SPARK_HOME: $SPARK_HOME"
        echo "SPARKLIB: $SPARKLIB"
        echo "FINK_ALERT_SIMULATOR: $FINK_ALERT_SIMULATOR"
        echo "KAFKA_HOME: $KAFKA_HOME"
        echo "PYTHONPATH: $PYTHONPATH"
        echo "JAVA_HOME: $JAVA_HOME"
        echo `python -V`
    - name: Run test suites
      run: |
        git fetch --unshallow --quiet
        fink init -c ${FINK_HOME}/conf/fink.conf.travis
        fink_kafka start
        fink_kafka --create-topic fink_outstream
        fink_simulator --docker -c ${FINK_HOME}/conf/fink_alert_simulator.conf
        fink_test
        bash <(curl -s https://codecov.io/bash)
    - uses: act10ns/slack@v1
      with:
        status: ${{ job.status }}
      if: always()
