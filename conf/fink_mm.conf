[CLIENT]
id=
secret=

[PATH]
# Prefix path on disk where are save GCN live data. used by the joining stream
online_gcn_data_prefix=${FINK_HOME}/datasim/gcn_alerts

# same path as online_gcn_data_prefix without the URI part. used by the gcn_stream monitor
hdfs_gcn_storage=/user/roman.le-montagner/gcn_storage/raw

# Prefix path on disk where are save ZTF live data.
# They can be in local FS (/path/ or files:///path/) or
# in distributed FS (e.g. hdfs:///path/).
# Be careful though to have enough disk space!
online_ztf_data_prefix=${FINK_HOME}

# Prefix path on disk to save GRB join ZTF data (work for both online and offline).
online_grb_data_prefix=${FINK_HOME}/fink_mm_test

# Path where are store the hbase catalog in order to query the hbase database
# DEPRECATED
hbase_catalog=/path/to/hbase/catalog

# HDFS configuration to read or write data on a hdfs cluster
# host are the IP adress of the hdfs driver
# port are the port where the hdfs driver listen
# user are the name of the hdfs user
[HDFS]
host=127.0.0.1
port=
user=


# Prior alert filter before the join
# ast_dist are the maximum association distance between an alert and a known asteroid (ssnamenr in arcsecond)

# pansstar_dist are the maximum association distance between an alert and a known source in Pansstar (ssdistnr in arcsecond)

# pansstar_star_score are the SExtractor score between 0 (Galaxy like) and 1 (Star like).
# The system filters to keep alerts close to a galaxy.

# gaia_dist are the maximum association distance between an alert and a known source in Pansstar (neargaia in arcsecond)
[PRIOR_FILTER]
ast_dist=5
pansstar_dist=2
pansstar_star_score=0.5
gaia_dist=5

[STREAM]
tinterval=30
manager=local[8]
driver_host=$HOSTNAME
principal=
secret=
role=
exec_env=/path/to/user/
driver_memory=4
executor_memory=8
max_core=16
executor_core=8
external_python_libs=
jars=
packages=org.apache.spark:spark-streaming-kafka-0-10-assembly_2.12:3.4.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,org.apache.spark:spark-avro_2.12:3.4.1,org.apache.hbase:hbase-shaded-mapreduce:2.2.7
external_files=

# Kafka Broker configuration
# kafka_broker ar ethe IP adress
# username and password is required to distribute data
[DISTRIBUTION]
kafka_broker=localhost:9092
username_writer=toto
password_writer=tata

[ADMIN]
debug=True
# Healpix map resolution, better if a power of 2
NSIDE=4

[OFFLINE]
time_window=7
